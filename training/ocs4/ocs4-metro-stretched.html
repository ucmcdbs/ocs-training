<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>OpenShift Metro Disaster Recovery using Stretch Cluster :: OCS Training</title>
    <link rel="canonical" href="https://red-hat-storage.github.io/ocs-training/training/ocs4/ocs4-metro-stretched.html">
    <meta name="generator" content="Antora 2.3.4">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LGCEEZGN54"></script>
    <script>function gtag(){dataLayer.push(arguments)};window.dataLayer=window.dataLayer||[];gtag('js',new Date());gtag('config','G-LGCEEZGN54')</script>
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">
          <img src="../../_/img/header_logo_reverse.svg" height="40px" alt="Red Hat Data Services">
      </a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Get Help</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage" target="_blank">OCS Documentation</a>
            <a class="navbar-item" href="https://bugzilla.redhat.com/describecomponents.cgi?product=Red%20Hat%20OpenShift%20Container%20Storage" target="_blank">Browse Bugs</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Improve Guides</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://github.com/red-hat-storage/ocs-training/blob/master/CONTRIBUTING.adoc" target="_blank">Guidelines</a>
            <a class="navbar-item" href="https://github.com/red-hat-storage/ocs-training/issues/new/choose" target="_blank">Open Issue</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">More Infos</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://www.redhat.com/en/blog/channel/red-hat-storage" target="_blank">Our Blog</a>
            <a class="navbar-item" href="https://www.youtube.com/channel/UCoyG8VyvB-XUxQl1mD3T3Gw" target="_blank">Youtube</a>
            <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">OCS Technology</a>
          </div>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="training" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">OCS Installation and Configuration</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs.html">General deploy and use</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-install-no-ui.html">CLI based install</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-install-no-ui-1scale.html">Single node scaling support</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../../RegionalDR/manual/ocs4-multisite-replication.html">Regional disaster recovery (manual method)</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../../RegionalDR/helper/requirements.html">Regional disaster recovery (RDRhelper)</a>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="ocs4-metro-stretched.html">Metro disaster recovery</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-encryption.html">External KMS Encryption</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-cluster-downsize.html">Downsize existing OCS cluster</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-enable-rgw.html">Use RGW in OCS deployment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../infra-nodes/ocs4-infra-nodes.html">Deploying on Infra nodes</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../ocs4perf/ocs4perf.html">Test deployment post-install</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">OCS Installation and Configuration</span>
    <span class="version">master</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">OCS Installation and Configuration</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component">
      <a class="title" href="../../RegionalDR/index.html">ODF Regional DR</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../../RegionalDR/index.html">master</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">OCS Installation and Configuration</a></li>
    <li><a href="ocs4-metro-stretched.html">Metro disaster recovery</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="https://github.com/red-hat-storage/ocs-training/edit/master/training/modules/ocs4/pages/ocs4-metro-stretched.adoc">Edit this Page</a></div>
  </div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">OpenShift Metro Disaster Recovery using Stretch Cluster</h1>
<div id="toc" class="toc">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_overview">1. Overview</a></li>
<li><a href="#_metro_dr_stretch_cluster_prerequisites">2. Metro DR Stretch Cluster Prerequisites</a>
<ul class="sectlevel2">
<li><a href="#_apply_topology_zone_labels_to_ocp_nodes">2.1. Apply topology zone labels to OCP nodes</a></li>
</ul>
</li>
<li><a href="#_local_storage_operator">3. Local Storage Operator</a>
<ul class="sectlevel2">
<li><a href="#_installing_the_local_storage_operator_v4_7">3.1. Installing the Local Storage Operator v4.7</a></li>
</ul>
</li>
<li><a href="#_openshift_data_foundation_deployment">4. OpenShift Data Foundation Deployment</a>
<ul class="sectlevel2">
<li><a href="#_odf_operator_deployment">4.1. ODF Operator Deployment</a></li>
<li><a href="#_odf_storage_cluster_deployment">4.2. ODF Storage Cluster Deployment</a>
<ul class="sectlevel3">
<li><a href="#_validate_cluster_deployment">4.2.1. Validate Cluster Deployment</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_install_zone_aware_sample_application">5. Install Zone Aware Sample Application</a>
<ul class="sectlevel2">
<li><a href="#_modify_deployment_to_be_zone_aware">5.1. Modify Deployment to be Zone Aware</a></li>
</ul>
</li>
<li><a href="#_data_zone_failure_and_recovery">6. Data Zone Failure and Recovery</a>
<ul class="sectlevel2">
<li><a href="#_zone_failure">6.1. Zone Failure</a></li>
<li><a href="#_recovery_for_zone_aware_ha_applications_with_rwx_storage">6.2. Recovery for zone-aware HA applications with RWX storage</a></li>
<li><a href="#_recovery_for_ha_applications_with_rwx_storage">6.3. Recovery for HA applications with RWX storage</a></li>
<li><a href="#_recovery_for_applications_with_rwo_storage">6.4. Recovery for applications with RWO storage</a></li>
<li><a href="#_recovery_for_statefulset_pods">6.5. Recovery for StatefulSet pods</a></li>
</ul>
</li>
<li><a href="#_appendix_a_resiliency_for_openshift_registry_monitoring_routing">7. Appendix A: Resiliency for OpenShift Registry, Monitoring, Routing</a>
<ul class="sectlevel2">
<li><a href="#_openshift_registry_using_topologyspreadconstraints">7.1. OpenShift Registry using topologySpreadConstraints</a></li>
<li><a href="#_openshift_monitoring_using_label_placement">7.2. OpenShift Monitoring using label placement</a></li>
<li><a href="#_openshift_routers_using_label_placement">7.3. OpenShift Routers using label placement</a></li>
</ul>
</li>
<li><a href="#_appendix_b_metro_dr_storagecluster_for_cli_deployment">8. Appendix B: Metro DR StorageCluster for CLI deployment</a></li>
</ul>
</div>
<div class="sect1">
<h2 id="_overview"><a class="anchor" href="#_overview"></a>1. Overview</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In order to provide a level of disaster recovery an OpenShift Container Platform (OCP) deployment can be <code>stretched</code> between two geographically different locations. To be resilient in the face of a disaster necessary OCP services, including storage, must be able to survive when one of the two locations is partially or totally not available. This solution sometimes is called <code>Metro DR stretch cluster</code> which means the distance between the sites needs to be limited so that OpenShift services and storage can operate with acceptable performance.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Currently <code>Metro DR stretch cluster</code> is tested to be deployed where latencies do not exceed 4 milliseconds round-trip time (RTT) between OCP nodes in different locations. Contact <a href="https://access.redhat.com/support">Red Hat Customer Support</a> if you are planning to deploy with higher latencies.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The intent of this solution guide is to detail the steps and commands necessary to deploy <code>OpenShift Data Foundation</code>(ODF) along with Kubernetes zone topology labels to achieve a highly available storage infrastructure.</p>
</div>
<div class="paragraph">
<p>This is a general overview of the steps required to configure and execute <code>Metro DR stretch cluster</code> capabilities using ODF <strong>4.7</strong> or greater.</p>
</div>
<div class="olist arabic">
<ol class="arabic" start="1">
<li>
<p><strong>Ensure you have at least 3 OCP master nodes in 3 different locations</strong><br>
Ensure that there is a master node in each of the two locations and that the 3rd master node is in a 3rd location to act as the arbiter in the case of site outage.</p>
</li>
<li>
<p><strong>Ensure you have a minimum of 4 OCP worker nodes</strong><br>
Ensure you have at least 4 OCP worker nodes evenly dispersed across 2 different locations.</p>
</li>
<li>
<p><strong>Assign kubernetes topology zone labels to each master and worker node</strong><br>
Assign unique <code>topology.kubernetes.io/zone</code> labels to each node to define failure domains as well as the arbiter zone.</p>
</li>
<li>
<p><strong>Install ODF operators and storage cluster</strong><br>
Install the OpenShift Container Storage operator and storage cluster using OperatorHub in the OpenShift Web console.</p>
</li>
<li>
<p><strong>Install a sample application for failure testing</strong><br>
Install an OCP application that can be configured to be highly available with critical components deployed in both data zones.</p>
</li>
<li>
<p><strong>Power off all OCP nodes in one location</strong><br>
Simulate a site outage by powering off all nodes (including master node) and validate sample application availability.</p>
</li>
<li>
<p><strong>Power on all OCP nodes in failed location</strong><br>
Simulate a recovery by powering back on all nodes and validate sample application availability.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_metro_dr_stretch_cluster_prerequisites"><a class="anchor" href="#_metro_dr_stretch_cluster_prerequisites"></a>2. Metro DR Stretch Cluster Prerequisites</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The diagram shows the most simple deployment for <code>Metro DR stretch cluster</code>. The ODF Monitor pod that is the <code>arbiter</code> in case of a site outage can be scheduled on a master node. The diagram does not show the master nodes in each <code>Data Zone</code>. They are required for a highly available OCP control plane. Also, it is critical that the OCP nodes in one location have network reachability to the OCP nodes in the other two locations.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-metrodr-zones.png" alt="OpenShift nodes and ODF daemons">
</div>
<div class="title">Figure 1. OpenShift nodes and ODF daemons</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Refer to the <a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage/4.7/html/planning_your_deployment/index">planning guide</a> for node and storage sizing for ODF.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_apply_topology_zone_labels_to_ocp_nodes"><a class="anchor" href="#_apply_topology_zone_labels_to_ocp_nodes"></a>2.1. Apply topology zone labels to OCP nodes</h3>
<div class="paragraph">
<p>Prior to installing ODF the nodes used for storage and the node used to host the arbiter function must be specifically labeled to define their function. In our case we will use the label <code>datacenter1</code> for one location with storage nodes and <code>datacenter2</code> for the other location. The zone that will have the arbiter function in the case of a site outage will use the <code>arbiter</code> label. These labels are arbitrary but they do need to be unique for the 3 locations.</p>
</div>
<div class="paragraph">
<p>For example, you can label the nodes and in <code>Figure 1</code> as follows (including master nodes not shown in diagram):</p>
</div>
<div class="ulist">
<ul>
<li>
<p>topology.kubernetes.io/zone=arbiter for Master0</p>
</li>
<li>
<p>topology.kubernetes.io/zone=datacenter1 for Master1, Worker1, Worker2</p>
</li>
<li>
<p>topology.kubernetes.io/zone=datacenter2 for Master2, Worker3, Worker4</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To apply the labels to the node using <code>oc</code> CLI do the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>oc label node &lt;NODENAME&gt; topology.kubernetes.io/zone=&lt;LABLE&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>To validate the labels run the following commands using the example labels for the 3 zones:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get nodes -l topology.kubernetes.io/zone=arbiter -o name</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME
node/perf1-mz8bt-master-0</pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get nodes -l topology.kubernetes.io/zone=datacenter1 -o name</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME
node/perf1-mz8bt-master-1
node/perf1-mz8bt-worker-d2hdm
node/perf1-mz8bt-worker-k68rv</pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get nodes -l topology.kubernetes.io/zone=datacenter2 -o name</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME
node/perf1-mz8bt-master-2
node/perf1-mz8bt-worker-ntkp8
node/perf1-mz8bt-worker-qpwsr</pre>
</div>
</div>
<div class="paragraph">
<p>The <code>Metro DR stretch cluster</code> topology zone labels are now applied to the appropriate OCP nodes to define the three locations. Next step is installing the storage operators from OCP <strong>OperatorHub</strong>.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_local_storage_operator"><a class="anchor" href="#_local_storage_operator"></a>3. Local Storage Operator</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Now switch over to your <strong>Openshift Web Console</strong>. You can get your URL by
issuing command below to get the OCP 4 <code>console</code> route.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get -n openshift-console route console</code></pre>
</div>
</div>
<div class="paragraph">
<p>Copy the <strong>Openshift Web Console</strong> route to a browser tab and login using your cluster-admin username (i.e., kubeadmin) and password.</p>
</div>
<div class="sect2">
<h3 id="_installing_the_local_storage_operator_v4_7"><a class="anchor" href="#_installing_the_local_storage_operator_v4_7"></a>3.1. Installing the Local Storage Operator v4.7</h3>
<div class="paragraph">
<p>Once you are logged in, navigate to the <strong>Operators</strong> &#8594; <strong>OperatorHub</strong> menu.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS-OCP-OperatorHub.png" alt="OCP OperatorHub">
</div>
<div class="title">Figure 2. OCP OperatorHub</div>
</div>
<div class="paragraph">
<p>Now type <code>local storage</code> in the <strong>Filter by <em>keyword&#8230;&#8203;</em></strong> box.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-OCP-OperatorHub-LSOFilter.png" alt="OCP OperatorHub Filter">
</div>
<div class="title">Figure 3. OCP OperatorHub filter on OpenShift Data Foundation Operator</div>
</div>
<div class="paragraph">
<p>Select <code>Local Storage</code> and then select <strong>Install</strong>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-4.7-OCP4-OperatorHub-LSOInstall.png" alt="OCP OperatorHub Install">
</div>
<div class="title">Figure 4. OCP OperatorHub Install OpenShift Data Foundation</div>
</div>
<div class="paragraph">
<p>On the next screen make sure the settings are as shown in this figure.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-4.7-OCP4-OperatorHub-LSOSubscribe.png" alt="OCP OperatorHub Subscribe">
</div>
<div class="title">Figure 5. OCP Subscribe to OpenShift Data Foundation</div>
</div>
<div class="paragraph">
<p>Click <code>Install</code>.</p>
</div>
<div class="paragraph">
<p>Verify the Local Storage Operator deployment is successful.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get csv,pod -n openshift-local-storage</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>NAME                                                                                      DISPLAY         VERSION                 REPLACES   PHASE
clusterserviceversion.operators.coreos.com/local-storage-operator.4.7.0-202103270130.p0   Local Storage   4.7.0-202103270130.p0              Succeeded

NAME                                          READY   STATUS    RESTARTS   AGE
pod/local-storage-operator-5879cf9565-r5s7k   1/1     Running   0          31s</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
Do not proceed with the next instructions until the Local Storage Operator is deployed successfully.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_openshift_data_foundation_deployment"><a class="anchor" href="#_openshift_data_foundation_deployment"></a>4. OpenShift Data Foundation Deployment</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this section you will be installing ODF and enabling <code>arbiter</code> mode. For instruction specific to you environment reference <a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage/4.7/">ODF documentation</a>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Currently the <code>Metro DR stretch cluster</code> solution is only designed for use on VMware and Bare Metal servers.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The following will be installed:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The ODF Operator (OCS Operator in OCP Web console)</p>
</li>
<li>
<p>All other ODF resources (Ceph Pods, NooBaa Pods, StorageClasses)</p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="_odf_operator_deployment"><a class="anchor" href="#_odf_operator_deployment"></a>4.1. ODF Operator Deployment</h3>
<div class="paragraph">
<p>Start with creating the <code>openshift-storage</code> namespace.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc create namespace openshift-storage</code></pre>
</div>
</div>
<div class="paragraph">
<p>You must add the monitoring label to this namespace. This is required to get
prometheus metrics and alerts for the OCP storage dashboards. To label the
<code>openshift-storage</code> namespace use the following command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc label namespace openshift-storage "openshift.io/cluster-monitoring=true"</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The creation of the <code>openshift-storage</code> namespace, and the monitoring
label added to this namespace, can also be done during the OCS operator
installation using the <strong>Openshift Web Console</strong>.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Navigate to the <strong>Operators</strong> &#8594; <strong>OperatorHub</strong> menu again.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS-OCP-OperatorHub.png" alt="OCP OperatorHub">
</div>
<div class="title">Figure 6. OCP OperatorHub</div>
</div>
<div class="paragraph">
<p>Now type <code>openshift container storage</code> in the <strong>Filter by <em>keyword&#8230;&#8203;</em></strong> box.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-4.7-OCP-OperatorHub-Filter.png" alt="OCP OperatorHub Filter">
</div>
<div class="title">Figure 7. OCP OperatorHub filter on OpenShift Data Foundation Operator</div>
</div>
<div class="paragraph">
<p>Select <code>OpenShift Data Foundation Operator</code> and then select <strong>Install</strong>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-4.7-OCP4-OperatorHub-Install.png" alt="OCP OperatorHub Install">
</div>
<div class="title">Figure 8. OCP OperatorHub Install OpenShift Data Foundation</div>
</div>
<div class="paragraph">
<p>On the next screen make sure the settings are as shown in this figure.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-4.7-OCP4-OperatorHub-Subscribe.png" alt="OCP OperatorHub Subscribe">
</div>
<div class="title">Figure 9. OCP Subscribe to OpenShift Data Foundation</div>
</div>
<div class="paragraph">
<p>Click <code>Install</code>.</p>
</div>
<div class="paragraph">
<p>Now you can go back to your terminal window to check the progress of the
installation. Verify the operator is deployed successfully.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pods,csv -n openshift-storage</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>NAME                                        READY   STATUS    RESTARTS   AGE
pod/noobaa-operator-746ddfc79-fcrfz         1/1     Running   0          33s
pod/ocs-metrics-exporter-54b6d689f8-ltxvp   1/1     Running   0          32s
pod/ocs-operator-5bcdd97ff4-rgn7f           1/1     Running   0          33s
pod/rook-ceph-operator-7dd585bd97-sldkk     1/1     Running   0          33s

NAME                                                             DISPLAY                       VERSION        REPLACES   PHASE
clusterserviceversion.operators.coreos.com/ocs-operator.v4.7.0   OpenShift Container Storage   4.7.0                   Succeeded</pre>
</div>
</div>
<div class="admonitionblock caution">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
Reaching this status shows that the installation of your operator was successful. Reaching this state can take several minutes.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_odf_storage_cluster_deployment"><a class="anchor" href="#_odf_storage_cluster_deployment"></a>4.2. ODF Storage Cluster Deployment</h3>
<div class="paragraph">
<p>Navigate to the <strong>Operators</strong> &#8594; <strong>Installed Operators</strong> menu.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-4.7-OCP-InstalledOperators.png" alt="OCP OperatorHub">
</div>
<div class="title">Figure 10. Locate ODF Operator</div>
</div>
<div class="paragraph">
<p>Click on <code>Storage Cluster</code> as indicated in the graphic above.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-4.7-OCP-CreateStorageCluster.png" alt="ODF Storage Cluster">
</div>
<div class="title">Figure 11. ODF Storage Cluster</div>
</div>
<div class="paragraph">
<p>Click on <code>Create Storage Cluster</code> on the far right side.</p>
</div>
<div class="paragraph">
<p>Select the <strong>Internal - Attached Devices</strong> deployment option.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-4.7-OCP4-InternalAttached.png" alt="LSO Based Cluster">
</div>
<div class="title">Figure 12. Select LSO Based Cluster</div>
</div>
<div class="paragraph">
<p>Provide storage cluster details.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-4.7-OCP4-StorageClusterDetailsNew.png" alt="LSO Discovery Parameters">
</div>
<div class="title">Figure 13. LSO Discovery Parameters</div>
</div>
<div class="paragraph">
<p>Click <strong>Next</strong> at the bottom of the screen.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-4.7-OCP4-StorageClusterLSOConfiguration.png" alt="LSO Configuration Parameters">
</div>
<div class="title">Figure 14. LSO LocalVolumeSet and Storage Class Configuration</div>
</div>
<div class="paragraph">
<p>Enter the desired configuration for your Local Storage Operator and click <code>Next</code>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-4.7-OCP4-StorageClusterLSOStorageClass.png" alt="LSO Storage Class Confirmation">
</div>
<div class="title">Figure 15. LSO Storage Class Confirmation</div>
</div>
<div class="paragraph">
<p>Click <code>Yes</code> when asked to confirm the storage class creation.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
The local storage (LSO) configuration will take a few minute. Please be patient.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Next check the <code>Enable arbiter</code> checkbox. Select the correct topology zone
that is to receive the Arbiter Monitor. The zone label is <code>arbiter</code> in this case.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-4.7-OCP4-StorageClusterArbiterScreenNew.png" alt="Arbiter Mode Selection">
</div>
<div class="title">Figure 16. ODF Arbiter Mode Configuration</div>
</div>
<div class="paragraph">
<p>Select the LSO storage class you created as illustrated in the screen capture. Then click <code>Next</code>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-4.7-OCP4-ClassArbiterScreen.png" alt="ODF Storage Class Select">
</div>
<div class="title">Figure 17. ODF Storage Class Select</div>
</div>
<div class="paragraph">
<p>When asked if you want to enable encryption just click <strong>Next</strong> again.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You can combine cluster wide encryption with Arbiter mode during a real deployment.
It is not the topic of this particular exercise.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Review parameters and create the cluster.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-4.7-OCP4-StorageClusterReviewNew.png" alt="Review Cluster Parameters">
</div>
<div class="title">Figure 18. Review Cluster Parameters</div>
</div>
<div class="paragraph">
<p>Click <strong>Create</strong> at the bottom of the <code>Review storage cluster</code> window.</p>
</div>
<div class="sect3">
<h4 id="_validate_cluster_deployment"><a class="anchor" href="#_validate_cluster_deployment"></a>4.2.1. Validate Cluster Deployment</h4>
<div class="paragraph">
<p>Wait for your storage cluster to become operational. Do these steps to validate successful installation.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get cephcluster -n openshift-storage</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>NAME                             DATADIRHOSTPATH   MONCOUNT   AGE     PHASE   MESSAGE                        HEALTH
ocs-storagecluster-cephcluster   /var/lib/rook     5          4m55s   Ready   Cluster created successfully   HEALTH_OK</pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pods -n openshift-storage</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>NAME                                                              READY   STATUS      RESTARTS   AGE
csi-cephfsplugin-28n69                                            3/3     Running     0          5m34s
csi-cephfsplugin-5qfrr                                            3/3     Running     0          5m34s
csi-cephfsplugin-provisioner-6976556bd7-5nvzz                     6/6     Running     0          5m34s
csi-cephfsplugin-provisioner-6976556bd7-z2g7w                     6/6     Running     0          5m34s
csi-cephfsplugin-qwzbs                                            3/3     Running     0          5m34s
csi-cephfsplugin-wrrm5                                            3/3     Running     0          5m34s
csi-rbdplugin-44bxs                                               3/3     Running     0          5m35s
csi-rbdplugin-lzc2x                                               3/3     Running     0          5m35s
csi-rbdplugin-mdm4n                                               3/3     Running     0          5m35s
csi-rbdplugin-provisioner-6b8557bd8b-54kvr                        6/6     Running     0          5m35s
csi-rbdplugin-provisioner-6b8557bd8b-k24sd                        6/6     Running     0          5m35s
csi-rbdplugin-v66cl                                               3/3     Running     0          5m35s
noobaa-core-0                                                     1/1     Running     0          2m23s
noobaa-db-pg-0                                                    1/1     Running     0          2m23s
noobaa-endpoint-cf67f6789-tlmmg                                   1/1     Running     0          43s
noobaa-operator-746ddfc79-fcrfz                                   1/1     Running     0          66m
ocs-metrics-exporter-54b6d689f8-ltxvp                             1/1     Running     0          66m
ocs-operator-5bcdd97ff4-rgn7f                                     1/1     Running     0          66m
rook-ceph-crashcollector-ip-10-0-137-183-5859f89db8-56tzl         1/1     Running     0          4m20s
rook-ceph-crashcollector-ip-10-0-148-220-66d4b9868d-wpdgz         1/1     Running     0          4m37s
rook-ceph-crashcollector-ip-10-0-168-114-6dc89c87d8-l2ckg         1/1     Running     0          4m52s
rook-ceph-crashcollector-ip-10-0-172-31-58dd45f7b9-wfjjv          1/1     Running     0          5m8s
rook-ceph-crashcollector-ip-10-0-212-112-67bcbb8949-vpn6h         1/1     Running     0          4m5s
rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-64f7cb6dhb68v   2/2     Running     0          2m4s
rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-96fd85c5vcbhn   2/2     Running     0          2m3s
rook-ceph-mgr-a-55f6d78b6b-9nvzr                                  2/2     Running     0          3m4s
rook-ceph-mon-a-599568d496-cqfxb                                  2/2     Running     0          5m9s
rook-ceph-mon-b-5b56c99655-m69s2                                  2/2     Running     0          4m52s
rook-ceph-mon-c-5854699cbd-76lrv                                  2/2     Running     0          4m37s
rook-ceph-mon-d-765776ccfc-46qpn                                  2/2     Running     0          4m20s
rook-ceph-mon-e-6bdd6d6bb8-wxwkf                                  2/2     Running     0          4m5s
rook-ceph-operator-7dd585bd97-sldkk                               1/1     Running     0          66m
rook-ceph-osd-0-d75955974-qk5l9                                   2/2     Running     0          2m43s
rook-ceph-osd-1-7f886fd54-bgjzp                                   2/2     Running     0          2m42s
rook-ceph-osd-2-546d7986d-n52px                                   2/2     Running     0          2m42s
rook-ceph-osd-3-666b86f659-sln5d                                  2/2     Running     0          2m34s
rook-ceph-osd-prepare-ocs-deviceset-localblock-0-data-0ptfjctn6   0/1     Completed   0          3m3s
rook-ceph-osd-prepare-ocs-deviceset-localblock-1-data-0ffsr9kf5   0/1     Completed   0          3m2s
rook-ceph-osd-prepare-ocs-deviceset-localblock-2-data-0mzrl7rrl   0/1     Completed   0          3m2s
rook-ceph-osd-prepare-ocs-deviceset-localblock-3-data-0j7md76tl   0/1     Completed   0          3m1s</pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_install_zone_aware_sample_application"><a class="anchor" href="#_install_zone_aware_sample_application"></a>5. Install Zone Aware Sample Application</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this section the <code>ocs-storagecluster-cephfs</code> <strong>StorageClass</strong> will be used to
create a RWX (ReadWriteMany) <strong>PVC</strong> that can be used by multiple pods at the
same time. The application we will use is called <code>File Uploader</code>.</p>
</div>
<div class="paragraph">
<p>Because this application will share the same RWX volume for storing files we can demonstrate how an application can be spread across topology zones so that in the event of a site outage it is still available. This works for persistent data access as well because ODF storage configured for <code>Metro DR stretch cluster</code> is also zone aware and highly available.</p>
</div>
<div class="paragraph">
<p>Create a new project:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc new-project my-shared-storage</code></pre>
</div>
</div>
<div class="paragraph">
<p>Next deploy the example PHP application called <code>file-uploader</code>:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc new-app openshift/php:7.2-ubi8~https://github.com/christianh814/openshift-php-upload-demo --name=file-uploader</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Sample Output:</div>
<div class="content">
<pre>--&gt; Found image 4f2dcc0 (9 days old) in image stream "openshift/php" under tag "7.2-ubi8" for "openshift/php:7.2-
ubi8"

    Apache 2.4 with PHP 7.2
    -----------------------
    PHP 7.2 available as container is a base platform for building and running various PHP 7.2 applications and f
rameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynam
ically generated web pages. PHP also offers built-in database integration for several commercial and non-commerci
al database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common
use of PHP coding is probably as a replacement for CGI scripts.

    Tags: builder, php, php72, php-72

    * A source build using source code from https://github.com/christianh814/openshift-php-upload-demo will be cr
eated
      * The resulting image will be pushed to image stream tag "file-uploader:latest"
      * Use 'oc start-build' to trigger a new build

--&gt; Creating resources ...
    imagestream.image.openshift.io "file-uploader" created
    buildconfig.build.openshift.io "file-uploader" created
    deployment.apps "file-uploader" created
    service "file-uploader" created
--&gt; Success
    Build scheduled, use 'oc logs -f buildconfig/file-uploader' to track its progress.
    Application is not exposed. You can expose services to the outside world by executing one or more of the comm
ands below:
     'oc expose service/file-uploader'
    Run 'oc status' to view your app.</pre>
</div>
</div>
<div class="paragraph">
<p>Watch the build log and wait for the application to be deployed:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc logs -f bc/file-uploader -n my-shared-storage</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example Output:</div>
<div class="content">
<pre>Cloning "https://github.com/christianh814/openshift-php-upload-demo" ...

[...]

Generating dockerfile with builder image image-registry.openshift-image-regis
try.svc:5000/openshift/php@sha256:d97466f33999951739a76bce922ab17088885db610c
0e05b593844b41d5494ea
STEP 1: FROM image-registry.openshift-image-registry.svc:5000/openshift/php@s
ha256:d97466f33999951739a76bce922ab17088885db610c0e05b593844b41d5494ea
STEP 2: LABEL "io.openshift.build.commit.author"="Christian Hernandez &lt;christ
ian.hernandez@yahoo.com&gt;"       "io.openshift.build.commit.date"="Sun Oct 1 1
7:15:09 2017 -0700"       "io.openshift.build.commit.id"="288eda3dff43b02f7f7
b6b6b6f93396ffdf34cb2"       "io.openshift.build.commit.ref"="master"       "
io.openshift.build.commit.message"="trying to modularize"       "io.openshift
.build.source-location"="https://github.com/christianh814/openshift-php-uploa
d-demo"       "io.openshift.build.image"="image-registry.openshift-image-regi
stry.svc:5000/openshift/php@sha256:d97466f33999951739a76bce922ab17088885db610
c0e05b593844b41d5494ea"
STEP 3: ENV OPENSHIFT_BUILD_NAME="file-uploader-1"     OPENSHIFT_BUILD_NAMESP
ACE="my-shared-storage"     OPENSHIFT_BUILD_SOURCE="https://github.com/christ
ianh814/openshift-php-upload-demo"     OPENSHIFT_BUILD_COMMIT="288eda3dff43b0
2f7f7b6b6b6f93396ffdf34cb2"
STEP 4: USER root
STEP 5: COPY upload/src /tmp/src
STEP 6: RUN chown -R 1001:0 /tmp/src
STEP 7: USER 1001
STEP 8: RUN /usr/libexec/s2i/assemble
---&gt; Installing application source...
=&gt; sourcing 20-copy-config.sh ...
---&gt; 17:24:39     Processing additional arbitrary httpd configuration provide
d by s2i ...
=&gt; sourcing 00-documentroot.conf ...
=&gt; sourcing 50-mpm-tuning.conf ...
=&gt; sourcing 40-ssl-certs.sh ...
STEP 9: CMD /usr/libexec/s2i/run
STEP 10: COMMIT temp.builder.openshift.io/my-shared-storage/file-uploader-1:3
b83e447
Getting image source signatures

[...]

Writing manifest to image destination
Storing signatures
Successfully pushed image-registry.openshift-image-registry.svc:5000/my-share
d-storage/file-uploader@sha256:929c0ce3dcc65a6f6e8bd44069862858db651358b88065
fb483d51f5d704e501
Push successful</pre>
</div>
</div>
<div class="paragraph">
<p>The command prompt returns out of the tail mode once you see <em>Push successful</em>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This use of the <code>new-app</code> command directly asked for application code to
be built and did not involve a template. That is why it only created a <strong>single
Pod</strong> deployment with a <strong>Service</strong> and no <strong>Route</strong>.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Let&#8217;s make our application production ready by exposing it via a <code>Route</code> and
scale to 4 instances for high availability:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc expose svc/file-uploader -n my-shared-storage</code></pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc scale --replicas=4 deploy/file-uploader -n my-shared-storage</code></pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pods -n my-shared-storage</code></pre>
</div>
</div>
<div class="paragraph">
<p>You should have 4 <code>file-uploader</code> <strong>Pods</strong> in a few minutes. Repeat the command
above until there are 4 <code>file-uploader</code> <strong>Pods</strong> in <code>Running</code> STATUS.</p>
</div>
<div class="paragraph">
<p>You can create a <strong>PersistentVolumeClaim</strong> and attach it into an application with
the <code>oc set volume</code> command. Execute the following</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc set volume deploy/file-uploader --add --name=my-shared-storage \
-t pvc --claim-mode=ReadWriteMany --claim-size=10Gi \
--claim-name=my-shared-storage --claim-class=ocs-storagecluster-cephfs \
--mount-path=/opt/app-root/src/uploaded \
-n my-shared-storage</code></pre>
</div>
</div>
<div class="paragraph">
<p>This command will:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>create a <strong>PersistentVolumeClaim</strong></p>
</li>
<li>
<p>update the <strong>Deployment</strong> to include a <code>volume</code> definition</p>
</li>
<li>
<p>update the <strong>Deployment</strong> to attach a <code>volumemount</code> into the specified
<code>mount-path</code></p>
</li>
<li>
<p>cause a new deployment of the 3 application <strong>Pods</strong></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Now, let&#8217;s look at the result of adding the volume:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pvc -n my-shared-storage</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example Output:</div>
<div class="content">
<pre>NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                AGE
my-shared-storage   Bound    pvc-5402cc8a-e874-4d7e-af76-1eb05bd2e7c7   10Gi       RWX            ocs-storagecluster-cephfs   52s</pre>
</div>
</div>
<div class="paragraph">
<p>Notice the <code>ACCESSMODE</code> being set to <strong>RWX</strong> (short for <code>ReadWriteMany</code>).</p>
</div>
<div class="paragraph">
<p>All 4 <code>file-uploader</code><strong>Pods</strong> are using the same <strong>RWX</strong> volume. Without this
<code>ACCESSMODE</code>, OpenShift will not attempt to attach multiple <strong>Pods</strong> to the
same <strong>PersistentVolume</strong> reliably. If you attempt to scale up deployments that
are using <strong>RWO</strong> or <code>ReadWriteOnce</code> storage, the <strong>Pods</strong> will actually all
become co-located on the same node.</p>
</div>
<div class="sect2">
<h3 id="_modify_deployment_to_be_zone_aware"><a class="anchor" href="#_modify_deployment_to_be_zone_aware"></a>5.1. Modify Deployment to be Zone Aware</h3>
<div class="paragraph">
<p>Currently the <code>file-upoader</code> <strong>Deployment</strong> is not zone aware and could schedule all of the <strong>Pods</strong> in the same zone. If this happened and there was a site outage then the application would be unavailable.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get deployment file-uploader -o yaml -n my-shared-storage | less</code></pre>
</div>
</div>
<div class="paragraph">
<p>Search for <code>containers</code> and repeat the search a few times until your output is similar. There is currently no pod placement rules in the default <strong>Deployment</strong> <code>file-uploader</code>.</p>
</div>
<div class="listingblock">
<div class="title">Example Output:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">[...]
spec:
  progressDeadlineSeconds: 600
  replicas: 4
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      deployment: file-uploader
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        openshift.io/generated-by: OpenShiftNewApp
      creationTimestamp: null
      labels:
        deployment: file-uploader
      spec:  # &lt;-- Start inserted lines after here
        containers:  # &lt;-- End inserted lines before here
        - image: image-registry.openshift-image-registry.svc:5000/my-shared-storage/file-uploader@sha256:a458ea62f990e431ad7d5f84c89e2fa27bdebdd5e29c5418c70c56eb81f0a26b
          imagePullPolicy: IfNotPresent
          name: file-uploader
[...]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Currently the deployment is not configured to be zone aware. The <strong>Deployment</strong> needs to be modified to use the topology zone labels as shown below. Edit the deployment and add the new lines below between the <code>start</code> and <code>end</code> point.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc edit deployment file-uploader -n my-shared-storage</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">[...]
      spec:
        topologySpreadConstraints:
          - labelSelector:
              matchLabels:
                deployment: file-uploader
            maxSkew: 1
            topologyKey: topology.kubernetes.io/zone
            whenUnsatisfiable: DoNotSchedule
          - labelSelector:
               matchLabels:
                 deployment: file-uploader
            maxSkew: 1
            topologyKey: kubernetes.io/hostname
            whenUnsatisfiable: ScheduleAnyway
        nodeSelector:
          node-role.kubernetes.io/worker: ""
        containers:
[...]</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>deployment.apps/file-uploader edited</pre>
</div>
</div>
<div class="paragraph">
<p>Now scale the deployment to zero <strong>Pods</strong>. and then back to 4 <strong>Pods</strong>. This is needed because the deployment changed in terms of <strong>Pod</strong> placement.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc scale deployment file-uploader --replicas=0 -n my-shared-storage</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>deployment.apps/file-uploader scaled</pre>
</div>
</div>
<div class="paragraph">
<p>And then back to 4 <strong>Pods</strong>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc scale deployment file-uploader --replicas=4 -n my-shared-storage</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>deployment.apps/file-uploader scaled</pre>
</div>
</div>
<div class="paragraph">
<p>Validate now that the 4 <strong>Pods</strong> are spread across the 4 nodes in <code>datacenter1</code> and <code>datacenter2</code> zones.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pods -o wide -n my-shared-storage | egrep '^file-uploader'| grep -v build | awk '{print $7}' | sort | uniq -c</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>   1 perf1-mz8bt-worker-d2hdm
   1 perf1-mz8bt-worker-k68rv
   1 perf1-mz8bt-worker-ntkp8
   1 perf1-mz8bt-worker-qpwsr</pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get nodes -L topology.kubernetes.io/zone | grep datacenter | grep -v master</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>perf1-mz8bt-worker-d2hdm   Ready    worker   35d   v1.20.0+5fbfd19   datacenter1
perf1-mz8bt-worker-k68rv   Ready    worker   35d   v1.20.0+5fbfd19   datacenter1
perf1-mz8bt-worker-ntkp8   Ready    worker   35d   v1.20.0+5fbfd19   datacenter2
perf1-mz8bt-worker-qpwsr   Ready    worker   35d   v1.20.0+5fbfd19   datacenter2</pre>
</div>
</div>
<div class="paragraph">
<p>Now let&#8217;s use the file uploader web application using your browser to upload
new files.</p>
</div>
<div class="paragraph">
<p>First, find the <strong>Route</strong> that has been created:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get route file-uploader -n my-shared-storage -o jsonpath --template="http://{.spec.host}{'\n'}"</code></pre>
</div>
</div>
<div class="paragraph">
<p>This will return a route similar to this one.</p>
</div>
<div class="listingblock">
<div class="title">Sample Output:</div>
<div class="content">
<pre>http://file-uploader-my-shared-storage.apps.cluster-ocs4-abdf.ocs4-abdf.sandbox744.opentlc.com</pre>
</div>
</div>
<div class="paragraph">
<p>Point your browser to the web application using your route above. <strong>Your <code>route</code>
will be different.</strong></p>
</div>
<div class="paragraph">
<p>The web app simply lists all uploaded files and offers the ability to upload
new ones as well as download the existing data. Right now there is
nothing.</p>
</div>
<div class="paragraph">
<p>Select an arbitrary file from your local machine and upload it to the app.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/uploader_screen_upload.png" alt="uploader screen upload">
</div>
<div class="title">Figure 19. A simple PHP-based file upload tool</div>
</div>
<div class="paragraph">
<p>Once done click <strong><em>List uploaded files</em></strong> to see the list of all currently
uploaded files.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_data_zone_failure_and_recovery"><a class="anchor" href="#_data_zone_failure_and_recovery"></a>6. Data Zone Failure and Recovery</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Given one of the important goals of the <code>Metro DR stretch cluster</code> is to provide resiliency in the face of a complete or partial site outage, it is important to understand the different methods of recovery for applications and their storage.</p>
</div>
<div class="paragraph">
<p>How the application is architected will determine how soon it can be available again on the active zone.</p>
</div>
<div class="sect2">
<h3 id="_zone_failure"><a class="anchor" href="#_zone_failure"></a>6.1. Zone Failure</h3>
<div class="paragraph">
<p>For purposes of this section we will consider a zone failure to be a failure where all OCP nodes, masters and workers, in a zone are no longer communicating with the resources in the second data zone (e.g., nodes powered down). If communication between data zones is partially still working (intermittent down/up), steps should be taken by cluster, storage, network admins to sever the communication path between the data zones for disaster recovery to succeed.</p>
</div>
</div>
<div class="sect2">
<h3 id="_recovery_for_zone_aware_ha_applications_with_rwx_storage"><a class="anchor" href="#_recovery_for_zone_aware_ha_applications_with_rwx_storage"></a>6.2. Recovery for zone-aware HA applications with RWX storage</h3>
<div class="paragraph">
<p>Applications that are deployed with <code>topologyKey: topology.kubernetes.io/zone</code>, have one or more replicas scheduled in each data zone, and are using shared storage (i.e., RWX cephfs volume) will recover on the active zone within 30-60 seconds for new connections. The short pause is for HAProxy to refresh connections if a router pod is now offline in the failed data zone. An example of this type of application is detailed in the <a href="#_install_zone_aware_sample_application">Install Zone Aware Sample Application</a> section.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you installed the <code>Sample Application</code> it is recommended to test the failure of a data zone by powering off the OCP nodes (at least the nodes with ODF storage devices) to validate that your <code>file-uploader</code> application is available and that new files can be uploaded.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_recovery_for_ha_applications_with_rwx_storage"><a class="anchor" href="#_recovery_for_ha_applications_with_rwx_storage"></a>6.3. Recovery for HA applications with RWX storage</h3>
<div class="paragraph">
<p>Applications that are using <code>topologyKey: kubernetes.io/hostname</code> or no topology configuration whatsoever, have no protection against all of the application replicas being in the same zone.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This can happen even with <strong><em>podAntiAffinity</em></strong> and <strong><em>topologyKey: kubernetes.io/hostname</em></strong> in the <strong>Pod</strong> spec because this anti-affinity rule is host-based and not zone-based.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>If this happens, all replicas in the same zone, and there is a zone failure the application using RWX storage will take 6-8 minutes to recover on the active zone. This pause is for the OCP nodes in the failed zone to become <code>NotReady</code> (60 seconds) and then for the default pod eviction timeout to expire (300 seconds).</p>
</div>
</div>
<div class="sect2">
<h3 id="_recovery_for_applications_with_rwo_storage"><a class="anchor" href="#_recovery_for_applications_with_rwo_storage"></a>6.4. Recovery for applications with RWO storage</h3>
<div class="paragraph">
<p>Applications that use RWO storage (ReadWriteOnce) have a know behavior described in this <a href="https://github.com/kubernetes/kubernetes/issues/65392">Kubernetes issue</a>. Because of this issue, if there is a data zone failure any application <strong>Pods</strong> in that zone mounting RWO volumes (i.e., cephrbd) are stuck in <code>Terminating</code> after 6-8 minutes and will not be recreated on the active zone without manual intervention.</p>
</div>
<div class="paragraph">
<p>To get the <code>Terminating</code> <strong>Pods</strong> to recreate on the active zone do one of these two actions:</p>
</div>
<div class="olist arabic">
<ol class="arabic" start="1">
<li>
<p><strong>Force delete the pod</strong><br>
Force deletions do not wait for confirmation from the kubelet that the Pod has been terminated.</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre>oc delete pod &lt;PODNAME&gt; --grace-period=0 --force --namespace &lt;NAMESPACE&gt;</pre>
</div>
</div>
<div class="olist arabic">
<ol class="arabic" start="2">
<li>
<p><strong>Delete the finalizer on the associated PV</strong><br>
Find the associated PV for the PVC that is mounted by the <code>Terminating</code> pod and delete the finalizer using <code>oc edit</code> or <code>oc patch</code> command. An easy way to find the associated <strong>PV</strong> is to describe the <code>Terminating</code>  pod. If you see a <code>Multi-Attach</code> warning it should have the <strong>PV</strong> names in the warning (i.e., pvc-0595a8d2-683f-443b-aee0-6e547f5f5a7c).</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre>oc describe pod &lt;PODNAME&gt; --namespace &lt;NAMESPACE&gt;</pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>[...]
Events:
  Type     Reason                  Age   From                     Message
  ----     ------                  ----  ----                     -------
  Normal   Scheduled               4m5s  default-scheduler        Successfully assigned openshift-storage/noobaa-db-pg-0 to perf1-mz8bt-worker-d2hdm
  Warning  FailedAttachVolume      4m5s  attachdetach-controller  Multi-Attach error for volume "pvc-0595a8d2-683f-443b-aee0-6e547f5f5a7c" Volume is already exclusively attached to one node and can't be attached to another</pre>
</div>
</div>
<div class="admonitionblock caution">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
OCP nodes with a status of <code>NotReady</code> may have an issue that prevents them from communicating with the OpenShift control plane. They may still be performing IO against persistent volumes in spite of this communication issue. If two pods are concurrently writing to the same RWO volume, there is a risk of data corruption. Some measure must be taken to ensure that processes on the <code>NotReady</code> node are terminated or blocked until they can be terminated. Using an out of band management system to power off a node, with confirmation, would be an example of ensuring process termination. Withdrawing a network route that is used by nodes at a failed site to communicate with storage would be an example of blocking. Note that before restoring service to the failed zone or nodes, there must be confirmation that all pods with persistent volumes have terminated successfully.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Once one of these two actions are done the application <strong>Pod</strong> should recreate on the active zone and mount its RWO storage.</p>
</div>
</div>
<div class="sect2">
<h3 id="_recovery_for_statefulset_pods"><a class="anchor" href="#_recovery_for_statefulset_pods"></a>6.5. Recovery for StatefulSet pods</h3>
<div class="paragraph">
<p><strong>Pods</strong> that are part of a stateful set have a similar issue as <strong>Pods</strong> mounting RWO volumes. Reference Kubernetes resource <a href="https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/#statefulset-considerations">StatefulSet considerations</a> for more information. To get the <strong>Pods</strong> part of a <strong>StatefulSet</strong> to recreate on the active zone after 6-8 minutes, the <strong>Pod</strong> needs to be force deleted with the same requirements (i.e., OCP node powered off or communication severed) as <strong>Pods</strong> with RWO volumes.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_appendix_a_resiliency_for_openshift_registry_monitoring_routing"><a class="anchor" href="#_appendix_a_resiliency_for_openshift_registry_monitoring_routing"></a>7. Appendix A: Resiliency for OpenShift Registry, Monitoring, Routing</h2>
<div class="sectionbody">
<div class="paragraph">
<p>As of OpenShift version 4.7 the out-of-the box services for the image registry, ingress routing, and monitoring (prometheus, alertmanger) are not configured for zone anti-affinity. They are configured in <strong>Deployments</strong> and <strong>StatefulSets</strong> to use <code>podAntiAffinity</code> with <code>topologyKey: kubernetes.io/hostname</code>. Also, given the anti-affinity is <code>preferredDuringSchedulingIgnoredDuringExecution</code> vs <code>requiredDuringSchedulingIgnoredDuringExecution</code>, that is no guarantee that <strong>Pods</strong> will even be scheduled on different hosts.</p>
</div>
<div class="sect2">
<h3 id="_openshift_registry_using_topologyspreadconstraints"><a class="anchor" href="#_openshift_registry_using_topologyspreadconstraints"></a>7.1. OpenShift Registry using topologySpreadConstraints</h3>
<div class="paragraph">
<p>As was described for our sample application the OCP image-registry can use <code>topologySpreadConstraints</code> in the <strong>Deployment</strong> to spread <strong>Pods</strong> between zones equally and then between nodes in a zone.</p>
</div>
<div class="paragraph">
<p>The <code>configs.imageregistry.operator.openshift.io</code> must be edited to use shared storage (i.e., RWX cephfs volume) to scale the number of image-registry <strong>Pods</strong>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Create the <code>ocs4registry</code> <strong>PVC</strong> of the desired size using the <code>ocs-storagecluster-cephfs</code> <strong>StorageClass</strong>.
</td>
</tr>
</table>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc edit configs.imageregistry.operator.openshift.io</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">[...]
  storage:
    managementState: Managed
    pvc:
      claim: ocs4registry
[...]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now the <code>configs.imageregistry.operator.openshift.io</code> must be configured to be <code>Unmanaged</code> so the <strong>Deployment</strong> changes will not reconcile immediately back to default configuration.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{"spec":{"managementState":"Unmanaged"}}'</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once this is done, delete everything under <strong><em>affinity:</em></strong> between <code>spec</code> and <code>containers</code> in the image-registry deployment and replace with the following:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc edit deployment image-registry -n openshift-image-registry</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">[...]
      spec:
        topologySpreadConstraints:
          - labelSelector:
              matchLabels:
                docker-registry: default
            maxSkew: 1
            topologyKey: topology.kubernetes.io/zone
            whenUnsatisfiable: DoNotSchedule
          - labelSelector:
               matchLabels:
                 docker-registry: default
            maxSkew: 1
            topologyKey: kubernetes.io/hostname
            whenUnsatisfiable: ScheduleAnyway
        nodeSelector:
          node-role.kubernetes.io/worker: ""
        containers:
[...]</code></pre>
</div>
</div>
<div class="paragraph">
<p>To schedule the <strong>Pods</strong> equally between zones you will need to scale the replicas for the <code>image-registry</code> <strong>Deployment</strong> to <code>1</code> and then back to the desired number of <strong>Pods</strong>. Verify the <strong>Pods</strong> are equally scheduled between zones.</p>
</div>
</div>
<div class="sect2">
<h3 id="_openshift_monitoring_using_label_placement"><a class="anchor" href="#_openshift_monitoring_using_label_placement"></a>7.2. OpenShift Monitoring using label placement</h3>
<div class="paragraph">
<p>The prometheus and alertmanager <strong>Pods</strong> are both created with a <strong>StatefulSet</strong>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get statefulset -n openshift-monitoring</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                READY   AGE
alertmanager-main   3/3     54d
prometheus-k8s      2/2     54d</pre>
</div>
</div>
<div class="paragraph">
<p>Because of the reconciling back to default configuration (shown for prometheus) that uses host-based placement, this configuration cannot be deleted from the <strong>StatefulSet</strong> and replaced with <code>topologySpreadConstraints</code> as was done for the image-registry <strong>Deployment</strong>. The same <code>podAntiAffinity</code> is used in the alertmanager <strong>Statefulset</strong> which does not guarantee that the <strong>Pods</strong> will be scheduled in both zones.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc edit statefulset prometheus-k8s -n openshift-monitoring</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">[...]
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: prometheus
                  operator: In
                  values:
                  - k8s
              namespaces:
              - openshift-monitoring
              topologyKey: kubernetes.io/hostname
            weight: 100
      containers:
[...]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Therefore a different solution is needed to make monitoring resources highly available across zones. The method is to use node labeling and create a new (or modify existing) <strong>ConfigMap</strong> named <code>cluster-monitoring-config</code> for placement via a <code>nodeSelector</code> configuration.</p>
</div>
<div class="paragraph">
<p>First add the labels to the nodes in the different zones.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc label node {nodename1; zone1} prometheusk8s=true
oc label node {nodename2; zone2} prometheusk8s=true
oc label node {nodename1; zone1} alertmanager=true
oc label node {nodename2; zone2} alertmanager=true</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
A node is labeled in each zone. This is done so if a node in a zone fails there is node in a different zone for the <strong>Pods</strong> to schedule on after the <code>pod-eviction-timeout</code>.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Now create and then modify the <strong>ConfigMap</strong> <code>cluster-monitoring-config</code>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc -n openshift-monitoring create configmap cluster-monitoring-config</code></pre>
</div>
</div>
<div class="paragraph">
<p>Patch the <strong>ConfigMap</strong> with the labels for prometheus and alertmanager <strong>Pod</strong> placement.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc patch -n openshift-monitoring --type=merge --patch='{"data":{"config.yaml":"alertmanagerMain:\n  nodeSelector:\n    alertmanager: true\nprometheusK8s:\n  nodeSelector:\n    prometheusk8s: true\nk8sPrometheusAdapter:\n  nodeSelector:\n    prometheusk8s: true\n"}}' cm/cluster-monitoring-config</code></pre>
</div>
</div>
<div class="paragraph">
<p>Scale the replicas for both alertmanager and prometheus <strong>StatefulSet</strong> to <code>1</code> and then back to <code>3</code> and <code>2</code> respectively. Verify the <strong>Pods</strong> scheduled on the OCP nodes with the new labels.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If there is a zone failure or even a node failure that has the new label, the prometheus and/or alertmanager <strong>Pods</strong> can end up all in the same zone. Once all labeled nodes are restored to a healthy state delete the prometheus and/or alertmanager <strong>Pods</strong> one by one until you they are spread across both zones again on the labeled nodes.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_openshift_routers_using_label_placement"><a class="anchor" href="#_openshift_routers_using_label_placement"></a>7.3. OpenShift Routers using label placement</h3>
<div class="paragraph">
<p>The OCP routers have a similar issue as the monitoring <strong>StatefulSets</strong> for prometheus and alertmanager. In the case of the routers it is the <strong>Deployment</strong> that reconciles back to default host-based placement.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get deployment -n openshift-ingress</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME             READY   UP-TO-DATE   AVAILABLE   AGE
router-default   2/2     2            2           54d</pre>
</div>
</div>
<div class="paragraph">
<p>Given the <strong>Deployment</strong> can not be edited (and have the edits stay) the same method can be used for <strong>Pod</strong> placement as was done for prometheus and alertmanager <strong>Pods</strong>. Label the nodes in each zone and then patch the <strong>IngressController</strong> <code>default</code> to use the label as a <code>nodeSelector</code>.</p>
</div>
<div class="paragraph">
<p>First add the labels to the nodes in the different zones.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc label node {nodename1; zone1} routerplacement=true
oc label node {nodename2; zone2} routerplacement=true</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
A node is labeled in each zone. This is because there are two ingress router <strong>Pods</strong> and they must schedule on unique nodes (contrary to monitoring <strong>Pods</strong> that can schedule on the same node).
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Now patch the <strong>IngressController</strong> <code>default</code> with the labels for the router <strong>Pod</strong> placement.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc patch -n openshift-ingress-operator ingresscontroller.operator.openshift.io default --type json --patch  '[{ "op": "add", "path": "/spec/nodePlacement", value: { "nodeSelector": { "matchLabels": { "routerplacement": "true" }}}}]'</code></pre>
</div>
</div>
<div class="paragraph">
<p>Scale the replicas for the <code>router-default</code> <strong>Deployment</strong> to <code>1</code> and then back to <code>2</code>. Verify the router <strong>Pods</strong> are scheduled on the OCP nodes with the new labels.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_appendix_b_metro_dr_storagecluster_for_cli_deployment"><a class="anchor" href="#_appendix_b_metro_dr_storagecluster_for_cli_deployment"></a>8. Appendix B: Metro DR StorageCluster for CLI deployment</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Example <strong>StorageCluster</strong> CR for <code>Metro DR stretch cluster</code>. For each set of 4 OSDs increment the <code>count</code> by 1. Creating the ODF <code>StorageCluster</code> using <code>oc</code> CLI is a replacement for the method described earlier in this document using the <strong>OpenShift Web Console</strong> UI to do the deployment.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Under the <code>managedResources</code> section is the default setting of <code>manage</code> for OCS services (i.e., block, file, object using RGW, object using NooBaa). This means any changes to OCS <code>CustomResources</code> (CRs) will always reconcile back to default values. The other choices instead of <code>manage</code> are <code>init</code> and <code>ignore</code>. The setting of <code>init</code> for the service (i.e., cephBlockPools) will not reconcile back to default if changes are made to the CR. The setting of <code>ignore</code> will not deploy the particular service.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: ocs.openshift.io/v1
kind: StorageCluster
metadata:
  name: ocs-storagecluster
  namespace: openshift-storage
spec:
  arbiter:
    enable: true  # &lt;-- Enable arbiter mode for Metro Dr stretch cluster
  nodeTopologies:
    arbiterLocation: arbiter  # &lt;-- Modify to label for arbiter zone
  manageNodes: false
  resources: {}
  monDataDirHostPath: /var/lib/rook
  managedResources:
    cephBlockPools:
      reconcileStrategy: manage
    cephFilesystems:
      reconcileStrategy: manage
    cephObjectStoreUsers:
      reconcileStrategy: manage
    cephObjectStores:
      reconcileStrategy: manage
    snapshotClasses:
      reconcileStrategy: manage
    storageClasses:
      reconcileStrategy: manage
  multiCloudGateway:
    reconcileStrategy: manage
  storageDeviceSets:
  - count: 1  # &lt;-- For each set of 4 disks increment the count by 1
    dataPVCTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: "100Gi"  # &lt;-- Use value smaller than actual disk size
        storageClassName: localblock  # &lt;-- Modify to correct LSO storageclass
        volumeMode: Block
    name: ocs-deviceset
    placement: {}
    portable: false
    replica: 4  # &lt;-- Replica = 4 for volume and object storage
    resources: {}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Save contents above to <code>storagecluster-metrodr.yaml</code> file.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc create -f storagecluster-metrodr.yaml -n openshift-storage</code></pre>
</div>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">
      <img src="../../_/img/header_logo.svg" alt="Red Hat Data Services">
  </a>
</footer>
<script src="../../_/js/site.js"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
