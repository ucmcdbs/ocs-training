<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Deploying and Managing OpenShift Container Storage :: OCS Training</title>
    <link rel="canonical" href="https://red-hat-storage.github.io/ocs-training/training/ocs4/ocs.html">
    <meta name="generator" content="Antora 2.3.4">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LGCEEZGN54"></script>
    <script>function gtag(){dataLayer.push(arguments)};window.dataLayer=window.dataLayer||[];gtag('js',new Date());gtag('config','G-LGCEEZGN54')</script>
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">
          <img src="../../_/img/header_logo_reverse.svg" height="40px" alt="Red Hat Data Services">
      </a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Get Help</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage" target="_blank">OCS Documentation</a>
            <a class="navbar-item" href="https://bugzilla.redhat.com/describecomponents.cgi?product=Red%20Hat%20OpenShift%20Container%20Storage" target="_blank">Browse Bugs</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Improve Guides</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://github.com/red-hat-storage/ocs-training/blob/master/CONTRIBUTING.adoc" target="_blank">Guidelines</a>
            <a class="navbar-item" href="https://github.com/red-hat-storage/ocs-training/issues/new/choose" target="_blank">Open Issue</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">More Infos</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://www.redhat.com/en/blog/channel/red-hat-storage" target="_blank">Our Blog</a>
            <a class="navbar-item" href="https://www.youtube.com/channel/UCoyG8VyvB-XUxQl1mD3T3Gw" target="_blank">Youtube</a>
            <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">OCS Technology</a>
          </div>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="training" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">OCS Installation and Configuration</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="ocs.html">General deploy and use</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-install-no-ui.html">CLI based install</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-install-no-ui-1scale.html">Single node scaling support</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../../RegionalDR/manual/ocs4-multisite-replication.html">Regional disaster recovery (manual method)</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../../RegionalDR/helper/requirements.html">Regional disaster recovery (RDRhelper)</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-metro-stretched.html">Metro disaster recovery</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-encryption.html">External KMS Encryption</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-cluster-downsize.html">Downsize existing OCS cluster</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-enable-rgw.html">Use RGW in OCS deployment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../infra-nodes/ocs4-infra-nodes.html">Deploying on Infra nodes</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../ocs4perf/ocs4perf.html">Test deployment post-install</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">OCS Installation and Configuration</span>
    <span class="version">master</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">OCS Installation and Configuration</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component">
      <a class="title" href="../../RegionalDR/index.html">ODF Regional DR</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../../RegionalDR/index.html">master</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">OCS Installation and Configuration</a></li>
    <li><a href="ocs.html">General deploy and use</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="https://github.com/red-hat-storage/ocs-training/edit/master/training/modules/ocs4/pages/ocs.adoc">Edit this Page</a></div>
  </div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Deploying and Managing OpenShift Container Storage</h1>
<div class="sect1">
<h2 id="_lab_overview"><a class="anchor" href="#_lab_overview"></a>1. Lab Overview</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This module is for both system administrators and application developers
interested in learning how to deploy and manage OpenShift Container Storage
(OCS). In this module you will be using OpenShift Container Platform (OCP)
4.x and the OCS operator to deploy Ceph and the Multi-Cloud-Gateway (MCG) as
a persistent storage solution for OCP workloads.</p>
</div>
<div class="sect2">
<h3 id="_in_this_lab_you_will_learn_how_to"><a class="anchor" href="#_in_this_lab_you_will_learn_how_to"></a>1.1. In this lab you will learn how to</h3>
<div class="ulist">
<ul>
<li>
<p>Configure and deploy containerized Ceph and MCG</p>
</li>
<li>
<p>Validate deployment of containerized Ceph and MCG</p>
</li>
<li>
<p>Deploy the Rook toolbox to run Ceph and RADOS commands</p>
</li>
<li>
<p>Create an application using Read-Write-Once (RWO) PVC that is based on Ceph RBD</p>
</li>
<li>
<p>Create an application using Read-Write-Many (RWX) PVC that is based on CephFS</p>
</li>
<li>
<p>Use OCS for Prometheus and AlertManager storage</p>
</li>
<li>
<p>Use the MCG to create a bucket and use in an application</p>
</li>
<li>
<p>Add more storage to the Ceph cluster</p>
</li>
<li>
<p>Review OCS metrics and alerts</p>
</li>
<li>
<p>Use must-gather to collect support information</p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS-Pods-Diagram.png" alt="Showing OCS4 pods">
</div>
<div class="title">Figure 1. OpenShift Container Storage components</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you want more information about how Ceph works please review
<a href="#_introduction_to_ceph">Introduction to Ceph</a> section before starting the exercises in this
module.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="labexercises"><a class="anchor" href="#labexercises"></a>2. Deploy your storage backend using the OCS operator</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_scale_ocp_cluster_and_add_new_worker_nodes"><a class="anchor" href="#_scale_ocp_cluster_and_add_new_worker_nodes"></a>2.1. Scale OCP cluster and add new worker nodes</h3>
<div class="paragraph">
<p>In this section, you will first validate the OCP environment has 2 or 3 worker
nodes before increasing the cluster size by additional 3 worker nodes for OCS
resources. The <code>NAME</code> of your OCP nodes will be different than shown below.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get nodes -l node-role.kubernetes.io/worker -l '!node-role.kubernetes.io/master'</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                                        STATUS   ROLES    AGE    VERSION
ip-10-0-153-37.us-east-2.compute.internal   Ready    worker   4d4h   v1.19.0+9f84db3
ip-10-0-170-25.us-east-2.compute.internal   Ready    worker   4d4h   v1.19.0+9f84db3</pre>
</div>
</div>
<div class="paragraph">
<p>Now you are going to add 3 more OCP compute nodes to cluster using <strong>machinesets</strong>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get machinesets -n openshift-machine-api</code></pre>
</div>
</div>
<div class="paragraph">
<p>This will show you the existing <strong>machinesets</strong> used to create the 2 or 3 worker
nodes in the cluster already. There is a <strong>machineset</strong> for each of 3 AWS
Availability Zones (AZ). Your <strong>machinesets</strong> <code>NAME</code> will be different than
below. In the case of only 2 workers one of the <strong>machinesets</strong> will not have any
machines (i.e., DESIRED=0) created.</p>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                                        DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-ocs4-8613-bc282-worker-us-east-2a   1         1         1       1           4d4h
cluster-ocs4-8613-bc282-worker-us-east-2b   1         1         1       1           4d4h
cluster-ocs4-8613-bc282-worker-us-east-2c   0         0                             4d4h</pre>
</div>
</div>
<div class="paragraph">
<p>Create new <strong>MachineSets</strong> that will in turn create storage-specific nodes for
your OCP cluster in the AWS AZs:</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<strong>Make sure you do the next steps for finding and using your CLUSTERID</strong>
</td>
</tr>
</table>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">CLUSTERID=$(oc get machineset -n openshift-machine-api -o jsonpath='{.items[0].metadata.labels.machine\.openshift\.io/cluster-api-cluster}')
echo $CLUSTERID</code></pre>
</div>
</div>
<div class="paragraph">
<p>Similar to the infrastructure nodes lab, create new <strong>MachineSets</strong> that will run
storage-specific nodes for your OCP cluster:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/training/modules/ocs4/attachments/cluster-workerocs-us-east-2.yaml | sed -e "s/CLUSTERID/${CLUSTERID}/g" | oc apply -f -</code></pre>
</div>
</div>
<div class="paragraph">
<p>Check that you have new <strong>machines</strong> created.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get machines -n openshift-machine-api | egrep 'NAME|workerocs'</code></pre>
</div>
</div>
<div class="paragraph">
<p>They will be in <code>Provisioning</code> for sometime and eventually in a <code>Running</code>
PHASE. The <code>NAME</code> of your machines will be different than shown below.</p>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                                                 PHASE     TYPE          REGION      ZONE         AGE
cluster-ocs4-8613-bc282-workerocs-us-east-2a-g6cfz   Running   m5.4xlarge    us-east-2   us-east-2a   3m48s
cluster-ocs4-8613-bc282-workerocs-us-east-2b-2zdgx   Running   m5.4xlarge    us-east-2   us-east-2b   3m48s
cluster-ocs4-8613-bc282-workerocs-us-east-2c-gg7br   Running   m5.4xlarge    us-east-2   us-east-2c   3m48s</pre>
</div>
</div>
<div class="paragraph">
<p>You can see that the workerocs <strong>machines</strong> are also using the AWS EC2 instance
type <code>m5.4xlarge</code>. The <code>m5.4xlarge</code> instance type has 16 cpus and 64 GB memory.</p>
</div>
<div class="paragraph">
<p>Now you want to see if our new <strong>machines</strong> are added to the OCP cluster.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">watch "oc get machinesets -n openshift-machine-api | egrep 'NAME|workerocs'"</code></pre>
</div>
</div>
<div class="paragraph">
<p>This step could take more than 5 minutes. The result of this command needs to
look like below before you proceed. All new workerocs <strong>machinesets</strong> should
have an integer, in this case <code>1</code>, filled out for all rows and under columns
<code>READY</code> and <code>AVAILABLE</code>. The <code>NAME</code> of your <strong>machinesets</strong> will be different
than shown below.</p>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                                           DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-ocs4-8613-bc282-workerocs-us-east-2a   1         1         1       1           16m
cluster-ocs4-8613-bc282-workerocs-us-east-2b   1         1         1       1           16m
cluster-ocs4-8613-bc282-workerocs-us-east-2c   1         1         1       1           16m</pre>
</div>
</div>
<div class="paragraph">
<p>You can exit by pressing <span class="keyseq"><kbd>Ctrl</kbd>+<kbd>C</kbd></span>.</p>
</div>
<div class="paragraph">
<p>Now check to see that you have 3 new OCP worker nodes. The <code>NAME</code> of your OCP
nodes will be different than shown below.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get nodes -l node-role.kubernetes.io/worker -l '!node-role.kubernetes.io/master'</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                                         STATUS   ROLES    AGE    VERSION
ip-10-0-147-230.us-east-2.compute.internal   Ready    worker   14m    v1.19.0+9f84db3
ip-10-0-153-37.us-east-2.compute.internal    Ready    worker   4d4h   v1.19.0+9f84db3
ip-10-0-170-25.us-east-2.compute.internal    Ready    worker   4d4h   v1.19.0+9f84db3
ip-10-0-175-8.us-east-2.compute.internal     Ready    worker   14m    v1.19.0+9f84db3
ip-10-0-209-53.us-east-2.compute.internal    Ready    worker   14m    v1.19.0+9f84db3</pre>
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s check to make sure the new OCP nodes have the OCS label. This label was
added in the <code>workerocs</code> <strong>machinesets</strong> so every <strong>machine</strong> created using these
<strong>machinesets</strong> will have this label.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get nodes -l cluster.ocs.openshift.io/openshift-storage=</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                                         STATUS   ROLES    AGE   VERSION
ip-10-0-147-230.us-east-2.compute.internal   Ready    worker   15m   v1.19.0+9f84db3
ip-10-0-175-8.us-east-2.compute.internal     Ready    worker   15m   v1.19.0+9f84db3
ip-10-0-209-53.us-east-2.compute.internal    Ready    worker   15m   v1.19.0+9f84db3</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_installing_the_ocs_operator"><a class="anchor" href="#_installing_the_ocs_operator"></a>2.2. Installing the OCS operator</h3>
<div class="paragraph">
<p>In this section you will be using three of the worker OCP 4 nodes to deploy
OCS 4 using the OCS Operator in OperatorHub. The following will be installed:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>An OCS <strong>OperatorGroup</strong></p>
</li>
<li>
<p>An OCS <strong>Subscription</strong></p>
</li>
<li>
<p>All other OCS resources (Operators, Ceph Pods, NooBaa Pods, StorageClasses)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Start with creating the <code>openshift-storage</code> namespace.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc create namespace openshift-storage</code></pre>
</div>
</div>
<div class="paragraph">
<p>You must add the monitoring label to this namespace. This is required to get
prometheus metrics and alerts for the OCP storage dashboards. To label the
<code>openshift-storage</code> namespace use the following command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc label namespace openshift-storage "openshift.io/cluster-monitoring=true"</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The creation of the <code>openshift-storage</code> namespace, and the monitoring
label added to this namespace, can also be done during the OCS operator
installation using the <strong>Openshift Web Console</strong>.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Now switch over to your <strong>Openshift Web Console</strong>. You can get your URL by
issuing command below to get the OCP 4 <code>console</code> route.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get -n openshift-console route console</code></pre>
</div>
</div>
<div class="paragraph">
<p>Copy the <strong>Openshift Web Console</strong> route to a browser tab and login using your cluster-admin username (i.e., kubadmin) and password.</p>
</div>
<div class="paragraph">
<p>Once you are logged in, navigate to the <strong>Operators</strong> &#8594; <strong>OperatorHub</strong> menu.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS-OCP-OperatorHub.png" alt="OCP OperatorHub">
</div>
<div class="title">Figure 2. OCP OperatorHub</div>
</div>
<div class="paragraph">
<p>Now type <code>openshift container storage</code> in the <strong>Filter by <em>keyword&#8230;&#8203;</em></strong> box.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-OCP-OperatorHub-Filter.png" alt="OCP OperatorHub Filter">
</div>
<div class="title">Figure 3. OCP OperatorHub filter on OpenShift Container Storage Operator</div>
</div>
<div class="paragraph">
<p>Select <code>OpenShift Container Storage Operator</code> and then select <strong>Install</strong>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-OCP4-OperatorHub-Install.png" alt="OCP OperatorHub Install">
</div>
<div class="title">Figure 4. OCP OperatorHub Install OpenShift Container Storage</div>
</div>
<div class="paragraph">
<p>On the next screen make sure the settings are as shown in this figure.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-OCP4-OperatorHub-Subscribe.png" alt="OCP OperatorHub Subscribe">
</div>
<div class="title">Figure 5. OCP Subscribe to OpenShift Container Storage</div>
</div>
<div class="paragraph">
<p>Click <code>Install</code>.</p>
</div>
<div class="paragraph">
<p>Now you can go back to your terminal window to check the progress of the
installation.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">watch oc -n openshift-storage get csv</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                  DISPLAY                       VERSION   REPLACES   PHASE
ocs-operator.v4.6.0   OpenShift Container Storage   4.6.0                Succeeded</pre>
</div>
</div>
<div class="paragraph">
<p>You can exit by pressing <span class="keyseq"><kbd>Ctrl</kbd>+<kbd>C</kbd></span>.</p>
</div>
<div class="paragraph">
<p>The resource <code>csv</code> is a shortened word for
<code>clusterserviceversions.operators.coreos.com</code>.</p>
</div>
<div class="admonitionblock caution">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
<div class="title">Please wait until the operator <code>PHASE</code> changes to <code>Succeeded</code></div>
This will mark that the installation of your operator was
successful. Reaching this state can take several minutes.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>You will now also see new operator pods in <code>openshift-storage</code>
namespace:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc -n openshift-storage get pods</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                                   READY   STATUS    RESTARTS   AGE
noobaa-operator-698746cd47-sp6w9       1/1     Running   0          108s
ocs-metrics-exporter-78bc44687-pg4hk   1/1     Running   0          107s
ocs-operator-6d99bc6787-d7m9d          1/1     Running   0          108s
rook-ceph-operator-59f7fb95d6-sdjd8    1/1     Running   0          108s</pre>
</div>
</div>
<div class="paragraph">
<p>Now switch back to your <strong>Openshift Web Console</strong> for the remainder of the
installation for OCS 4.</p>
</div>
<div class="paragraph">
<p>Select <code>View Operator</code> in figure below to get to the OCS configuration screen.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-OCP4-View-Operator.png" alt="View Operator in openshift-storage namespacee">
</div>
<div class="title">Figure 6. View Operator in openshift-storage namespace</div>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-OCP4-config-screen-all.png" alt="OCS configuration screen">
</div>
<div class="title">Figure 7. OCS configuration screen</div>
</div>
<div class="paragraph">
<p>On the top of the OCS configuration screen, scroll over to the right and click
on <code>Storage Cluster</code> and then click on <code>Create Storage Cluster</code> to the far
right. If you do not see <code>Create Storage Cluster</code> refresh your browser window.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-OCP4-config-screen-storage-cluster.png" alt="Create Storage Cluster">
</div>
<div class="title">Figure 8. Create Storage Cluster</div>
</div>
<div class="paragraph">
<p>The <code>Create Storage Cluster</code> screen will display.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-config-screen-partial1.png" alt="Create Storage Cluster default settings">
</div>
<div class="title">Figure 9. Create Storage Cluster default settings</div>
</div>
<div class="paragraph">
<p>Leave the default selection of <code>Internal</code>, <code>gp2</code>, <code>2 TiB</code> and Encryption <code>Disabled</code>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-config-screen-partial2.png" alt="Create a new storage cluster">
</div>
<div class="title">Figure 10. Create a new storage cluster</div>
</div>
<div class="paragraph">
<p>There should be 3 worker nodes already selected that had the OCS label
applied in the last section. Execute command below and make sure they are all
selected.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get nodes --show-labels | grep ocs | cut -d ' ' -f1</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then click on the button <code>Create</code> below the dialog box with the 3 workers
selected with a <code>checkmark</code>.</p>
</div>
<div class="paragraph">
<p>You can watch the deployment using the <strong>Openshift Web Console</strong> by going
back to the <code>Openshift Container Storage Operator</code> screen and selecting <code>All
instances</code>.</p>
</div>
<div class="paragraph">
<p>Please wait until all <strong>Pods</strong> are marked as <code>Running</code> in the CLI or until you
see all instances shown below as <code>Ready</code> Status in the Web Console as shown in the following diagram:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-OCP4-finished-cluster-install.png" alt="OCS instance overview after cluster install is finished">
</div>
<div class="title">Figure 11. OCS instance overview after cluster install is finished</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc -n openshift-storage get pods</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Output when the cluster installation is finished</div>
<div class="content">
<pre>NAME                                                              READY   STATUS      RESTART
S   AGE
csi-cephfsplugin-875xd                                            3/3     Running     0
    23m
csi-cephfsplugin-bncsj                                            3/3     Running     0
    23m
csi-cephfsplugin-hjv77                                            3/3     Running     0
    23m
csi-cephfsplugin-lch4m                                            3/3     Running     0
    23m
csi-cephfsplugin-provisioner-6cfdc4bfbb-cklxs                     6/6     Running     0
    23m
csi-cephfsplugin-provisioner-6cfdc4bfbb-krkq5                     6/6     Running     0
    23m
csi-cephfsplugin-wtp4v                                            3/3     Running     0
    23m
csi-rbdplugin-7clqf                                               3/3     Running     0
    23m
csi-rbdplugin-8nllt                                               3/3     Running     0
    23m
csi-rbdplugin-d267h                                               3/3     Running     0
    23m
csi-rbdplugin-provisioner-b46dd5c7-vd58q                          6/6     Running     0
    23m
csi-rbdplugin-provisioner-b46dd5c7-z8mx6                          6/6     Running     0
    23m
csi-rbdplugin-tdj8f                                               3/3     Running     0
    23m
csi-rbdplugin-wp65b                                               3/3     Running     0
    23m
noobaa-core-0                                                     1/1     Running     0
    19m
noobaa-db-0                                                       1/1     Running     0
    19m
noobaa-endpoint-86cc5df669-ffqj2                                  1/1     Running     0
    16m
noobaa-operator-698746cd47-sp6w9                                  1/1     Running     0
    17h
ocs-metrics-exporter-78bc44687-pg4hk                              1/1     Running     0
    17h
ocs-operator-6d99bc6787-d7m9d                                     1/1     Running     0
    17h
rook-ceph-crashcollector-ip-10-0-147-230-7cbf854757-chlgs         1/1     Running     0
    20m
rook-ceph-crashcollector-ip-10-0-175-8-5779d5d5df-p6hkl           1/1     Running     0
    21m
rook-ceph-crashcollector-ip-10-0-209-53-7ccc4cc785-wjxzd          1/1     Running     0
    21m
rook-ceph-drain-canary-128c383c26627b938ab0fd7f47f58d33-665pbsg   1/1     Running     0
    19m
rook-ceph-drain-canary-84c954eec459013180f78efd0a35792c-7b6qdnj   1/1     Running     0
    19m
rook-ceph-drain-canary-ip-10-0-175-8.us-east-2.compute.intrh526   1/1     Running     0
    19m
rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-756df8b4kp9kr   1/1     Running     0
    18m
rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-64585764bbg6b   1/1     Running     0
    18m
rook-ceph-mgr-a-5c74bb4b85-5x26g                                  1/1     Running     0
    20m
rook-ceph-mon-a-746b5457c-hlh7n                                   1/1     Running     0
    21m
rook-ceph-mon-b-754b99cfd-xs9g4                                   1/1     Running     0
    21m
rook-ceph-mon-c-7474d96f55-qhhb6                                  1/1     Running     0
    20m
rook-ceph-operator-59f7fb95d6-sdjd8                               1/1     Running     0
    17h
rook-ceph-osd-0-7d45696497-jwgb7                                  1/1     Running     0
    19m
rook-ceph-osd-1-6f49b665c7-gxq75                                  1/1     Running     0
    19m
rook-ceph-osd-2-76ffc64cd-9zg65                                   1/1     Running     0
    19m
rook-ceph-osd-prepare-ocs-deviceset-gp2-0-data-0-9977n-49ngd      0/1     Completed   0
    20m
rook-ceph-osd-prepare-ocs-deviceset-gp2-1-data-0-nnmpv-z8vq6      0/1     Completed   0
    20m
rook-ceph-osd-prepare-ocs-deviceset-gp2-2-data-0-mtbtj-xrj2n      0/1     Completed   0
    20m</pre>
</div>
</div>
<div class="paragraph">
<p>The great thing about operators and OpenShift is that the operator has the
intelligence about the deployed components built-in. And, because of the
relationship between the <code>CustomResource</code> and the operator, you can check the
status by looking at the <code>CustomResource</code> itself. When you went therough the UI
dialogs, ultimately in the back-end an instance of a <code>StorageCluster</code> was
created:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get storagecluster -n openshift-storage</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can check the status of the storage cluster with the following:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get storagecluster -n openshift-storage ocs-storagecluster -o jsonpath='{.status.phase}{"\n"}'</code></pre>
</div>
</div>
<div class="paragraph">
<p>If it says <code>Ready</code>, you can continue.</p>
</div>
</div>
<div class="sect2">
<h3 id="_getting_to_know_the_storage_dashboards"><a class="anchor" href="#_getting_to_know_the_storage_dashboards"></a>2.3. Getting to know the Storage Dashboards</h3>
<div class="paragraph">
<p>You can now also check the status of your storage cluster with the OCS specific
<strong>Dashboards</strong> that are included in your <strong>Openshift Web Console</strong>. You can reach
this by clicking on <code>Overview</code> on your left navigation bar, then selecting
<code>Persistent Storage</code> on the top navigation bar of the content page.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-OCP4-Overview-Location.png" alt="Location of OCS Dashboards">
</div>
<div class="title">Figure 12. Location of OCS Dashboards</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you just finished your OCS 4 deployment it could take 5-10 minutes
for your <strong>Dashboards</strong> to fully populate. Different versions of OCP 4 may have minor differences in <strong>Dashboard</strong> sections and naming of <strong>Dashboards</strong>.
</td>
</tr>
</table>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS-dashboard-healthy.png" alt="Storage Dashboard after successful storage installation">
</div>
<div class="title">Figure 13. Storage Dashboard after successful storage installation</div>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 0%;">
<col style="width: 9.0909%;">
<col style="width: 90.9091%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>1</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Health</p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Quick overview of the general health of the storage cluster</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>2</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Details</p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Overview of the deployed storage cluster version and backend provider</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>3</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Inventory</p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>List of all the resources that are used and offered by the storage system</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>4</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Events</p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Live overview of all the changes that are being done affecting the storage cluster</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>5</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Utilization</p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Overview of the storage cluster usage and performance</p>
</div></div></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>OCS ships with a <strong>Dashboard</strong> for the Object Store service as well. From the <strong>Overview</strong> click on the <code>Object Service</code> on the top
navigation bar of the content page.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS-noobaa-dashboard-healthy.png" alt="OCS Multi-Cloud-Gateway Dashboard after successful installation">
</div>
<div class="title">Figure 14. OCS Multi-Cloud-Gateway Dashboard after successful installation</div>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 0%;">
<col style="width: 9.0909%;">
<col style="width: 90.9091%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>1</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Health</p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Quick overview of the general health of the Multi-Cloud-Gateway</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>2</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Details</p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Overview of the deployed MCG version and backend provider including a link to the MCG Console</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>3</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Buckets</p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>List of all the ObjectBucket with are offered and ObjectBucketClaims which are connected to them</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>4</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Resource Providers</p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Shows the list of configured Resource Providers that are available as backing storage in the MCG</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>5</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Counters</p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Shows the current numbers of reads and writes issued against each provider</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>6</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Events</p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Live overview of all the changes that are being done affecting the MCG</p>
</div></div></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Once this is all healthy, you will be able to use the three new
<strong>StorageClasses</strong> created during the OCS 4 Install:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>ocs-storagecluster-ceph-rbd</p>
</li>
<li>
<p>ocs-storagecluster-cephfs</p>
</li>
<li>
<p>openshift-storage.noobaa.io</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can see these three <strong>StorageClasses</strong> from the Openshift Web Console by
expanding the <code>Storage</code> menu in the left navigation bar and selecting
<code>Storage Classes</code>. You can also run the command below:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc -n openshift-storage get sc</code></pre>
</div>
</div>
<div class="paragraph">
<p>Please make sure the three storage classes are available in your cluster
before proceeding.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The NooBaa pod used the <code>ocs-storagecluster-ceph-rbd</code> storage class for
creating a PVC for mounting to the <code>db</code> container.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_using_the_rook_ceph_toolbox_to_check_on_the_ceph_backing_storage"><a class="anchor" href="#_using_the_rook_ceph_toolbox_to_check_on_the_ceph_backing_storage"></a>2.4. Using the Rook-Ceph toolbox to check on the Ceph backing storage</h3>
<div class="paragraph">
<p>Since the Rook-Ceph <strong>toolbox</strong> is not shipped with OCS, we need to deploy it
manually.</p>
</div>
<div class="paragraph">
<p>You can patch the <code>OCSInitialization ocsinit</code> using the following command line:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc patch OCSInitialization ocsinit -n openshift-storage --type json --patch  '[{ "op": "replace", "path": "/spec/enableCephTools", "value": true }]'</code></pre>
</div>
</div>
<div class="paragraph">
<p>After the <code>rook-ceph-tools</code> <strong>Pod</strong> is <code>Running</code> you can access the <strong>toolbox</strong>
like this:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once inside the <strong>toolbox</strong>, try out the following Ceph commands:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph status</code></pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd status</code></pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd tree</code></pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph df</code></pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">rados df</code></pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph versions</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>sh-4.2# ceph status
  cluster:
    id:     e3398039-f8c6-4937-ba9d-655f5c01e0ae
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum a,b,c (age 6h)
    mgr: a(active, since 6h)
    mds: ocs-storagecluster-cephfilesystem:1 {0=ocs-storagecluster-cephfilesystem-a=up:active} 1 up:standby-replay
    osd: 3 osds: 3 up (since 6h), 3 in (since 6h)

  task status:
    scrub status:
        mds.ocs-storagecluster-cephfilesystem-a: idle
        mds.ocs-storagecluster-cephfilesystem-b: idle

  data:
    pools:   3 pools, 96 pgs
    objects: 120 objects, 245 MiB
    usage:   3.5 GiB used, 6.0 TiB / 6 TiB avail
    pgs:     96 active+clean

  io:
    client:   853 B/s rd, 16 KiB/s wr, 1 op/s rd, 1 op/s wr</pre>
</div>
</div>
<div class="paragraph">
<p>You can exit the toolbox by either pressing <span class="keyseq"><kbd>Ctrl</kbd>+<kbd>D</kbd></span> or by executing exit.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">exit</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_create_a_new_ocp_application_deployment_using_ceph_rbd_volume"><a class="anchor" href="#_create_a_new_ocp_application_deployment_using_ceph_rbd_volume"></a>3. Create a new OCP application deployment using Ceph RBD volume</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this section the <code>ocs-storagecluster-ceph-rbd</code> <strong>StorageClass</strong> will be used
by an OCP application + database <strong>Deployment</strong> to create RWO (ReadWriteOnce)
persistent storage. The persistent storage will be a Ceph RBD (RADOS Block
Device) volume in the Ceph pool <code>ocs-storagecluster-cephblockpool</code>.</p>
</div>
<div class="paragraph">
<p>To do so we have created a template file, based on the OpenShift
rails-pgsql-persistent template, that includes an extra parameter STORAGE_CLASS
that enables the end user to specify the <strong>StorageClass</strong> the PVC should use.
Feel free to download
<code><a href="https://github.com/red-hat-storage/ocs-training/blob/master/training/modules/ocs4/attachments/configurable-rails-app.yaml" class="bare">https://github.com/red-hat-storage/ocs-training/blob/master/training/modules/ocs4/attachments/configurable-rails-app.yaml</a></code> to check on the format of this
template. Search for <code>STORAGE_CLASS</code> in the downloaded content.</p>
</div>
<div class="paragraph">
<p>Make sure that you completed all previous sections so that you are ready to
start the Rails + PostgreSQL <strong>Deployment</strong>.</p>
</div>
<div class="paragraph">
<p>Start by creating a new project:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc new-project my-database-app</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then use the <code>rails-pgsql-persistent</code> template to create the new application.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/training/modules/ocs4/attachments/configurable-rails-app.yaml | oc new-app -p STORAGE_CLASS=ocs-storagecluster-ceph-rbd -p VOLUME_CAPACITY=5Gi -f -</code></pre>
</div>
</div>
<div class="paragraph">
<p>After the deployment is started you can monitor with these commands.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc status</code></pre>
</div>
</div>
<div class="paragraph">
<p>Check the PVC is created.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pvc -n my-database-app</code></pre>
</div>
</div>
<div class="paragraph">
<p>This step could take 5 or more minutes. Wait until there are 2 <strong>Pods</strong> in
<code>Running</code> STATUS and 4 <strong>Pods</strong> in <code>Completed</code> STATUS as shown below.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">watch oc get pods -n my-database-app</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                                READY   STATUS      RESTARTS   AGE
postgresql-1-deploy                 0/1     Completed   0          5m48s
postgresql-1-lf7qt                  1/1     Running     0          5m40s
rails-pgsql-persistent-1-build      0/1     Completed   0          5m49s
rails-pgsql-persistent-1-deploy     0/1     Completed   0          3m36s
rails-pgsql-persistent-1-hook-pre   0/1     Completed   0          3m28s
rails-pgsql-persistent-1-pjh6q      1/1     Running     0          3m14s</pre>
</div>
</div>
<div class="paragraph">
<p>You can exit by pressing <span class="keyseq"><kbd>Ctrl</kbd>+<kbd>C</kbd></span>.</p>
</div>
<div class="paragraph">
<p>Once the deployment is complete you can now test the application and the
persistent storage on Ceph.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get route rails-pgsql-persistent -n my-database-app -o jsonpath --template="http://{.spec.host}/articles{'\n'}"</code></pre>
</div>
</div>
<div class="paragraph">
<p>This will return a route similar to this one.</p>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>http://rails-pgsql-persistent-my-database-app.apps.cluster-ocs4-8613.ocs4-8613.sandbox944.opentlc.com/articles</pre>
</div>
</div>
<div class="paragraph">
<p>Copy your route (different than above) to a browser window to create articles.</p>
</div>
<div class="paragraph">
<p>Enter the <code>username</code> and <code>password</code> below to create articles and comments.
The articles and comments are saved in a PostgreSQL database which stores its
table spaces on the Ceph RBD volume provisioned using the
<code>ocs-storagecluster-ceph-rbd</code> <strong>StorageClass</strong> during the application
deployment.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>username: openshift
password: secret</pre>
</div>
</div>
<div class="paragraph">
<p>Lets now take another look at the Ceph <code>ocs-storagecluster-cephblockpool</code>
created by the <code>ocs-storagecluster-ceph-rbd</code> <strong>StorageClass</strong>. Log into the
<strong>toolbox</strong> pod again.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD</code></pre>
</div>
</div>
<div class="paragraph">
<p>Run the same Ceph commands as before the application deployment and compare
to results in prior section. Notice the number of objects in
<code>ocs-storagecluster-cephblockpool</code> has increased. The third command lists
RBD volumes and we should now have two RBDs.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph df</code></pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">rados df</code></pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">rbd -p ocs-storagecluster-cephblockpool ls | grep vol</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can exit the toolbox by either pressing <span class="keyseq"><kbd>Ctrl</kbd>+<kbd>D</kbd></span> or by executing exit.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">exit</code></pre>
</div>
</div>
<div class="sect2">
<h3 id="_matching_pvs_to_rbds"><a class="anchor" href="#_matching_pvs_to_rbds"></a>3.1. Matching PVs to RBDs</h3>
<div class="paragraph">
<p>A handy way to match OCP persistent volumes (<strong>PVs</strong>)to Ceph RBDs is to execute:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pv -o 'custom-columns=NAME:.spec.claimRef.name,PVNAME:.metadata.name,STORAGECLASS:.spec.storageClassName,VOLUMEHANDLE:.spec.csi.volumeHandle'</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                      PVNAME                                     STORAGECLASS                  VOLUMEHANDLE
ocs-deviceset-0-0-d2ppm   pvc-2c08bd9c-332d-11ea-a32f-061f7a67362c   gp2                           &lt;none&gt;
ocs-deviceset-1-0-9tmc6   pvc-2c0a0ed5-332d-11ea-a32f-061f7a67362c   gp2                           &lt;none&gt;
ocs-deviceset-2-0-qtbfv   pvc-2c0babb3-332d-11ea-a32f-061f7a67362c   gp2                           &lt;none&gt;
db-noobaa-core-0          pvc-4610a3ce-332d-11ea-a32f-061f7a67362c   ocs-storagecluster-ceph-rbd   0001-0011-openshift-storage-0000000000000001-4a74e248-332d-11ea-9a7c-0a580a820205
postgresql                pvc-874f93cb-3330-11ea-90b1-0a10d22e734a   ocs-storagecluster-ceph-rbd   0001-0011-openshift-storage-0000000000000001-8765a21d-3330-11ea-9a7c-0a580a820205
rook-ceph-mon-a           pvc-d462ecb0-332c-11ea-a32f-061f7a67362c   gp2                           &lt;none&gt;
rook-ceph-mon-b           pvc-d79d0db4-332c-11ea-a32f-061f7a67362c   gp2                           &lt;none&gt;
rook-ceph-mon-c           pvc-da9cc0e3-332c-11ea-a32f-061f7a67362c   gp2                           &lt;none&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>The second half of the <code>VOLUMEHANDLE</code> column mostly matches what your RBD is
named inside of Ceph. All you have to do is append <code>csi-vol-</code> to the front
like this:</p>
</div>
<div class="listingblock execute">
<div class="title">Get the full RBD name and the associated information for your postgreSQL <strong>PV</strong></div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">CSIVOL=$(oc get pv $(oc get pv | grep my-database-app | awk '{ print $1 }') -o jsonpath='{.spec.csi.volumeHandle}' | cut -d '-' -f 6- | awk '{print "csi-vol-"$1}')
echo $CSIVOL</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Examplet output:</div>
<div class="content">
<pre>csi-vol-8765a21d-3330-11ea-9a7c-0a580a820205</pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD rbd -p ocs-storagecluster-cephblockpool info $CSIVOL</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>rbd image 'csi-vol-8765a21d-3330-11ea-9a7c-0a580a820205':
        size 5 GiB in 1280 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 17e811c7f287
        block_name_prefix: rbd_data.17e811c7f287
        format: 2
        features: layering
        op_features:
        flags:
        create_timestamp: Thu Jan  9 22:36:51 2020
        access_timestamp: Thu Jan  9 22:36:51 2020
        modify_timestamp: Thu Jan  9 22:36:51 2020</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_expand_rbd_based_pvcs"><a class="anchor" href="#_expand_rbd_based_pvcs"></a>3.2. Expand RBD based PVCs</h3>
<div class="paragraph">
<p>OpenShift 4.5 and later versions let you expand an existing PVC based on the
<code>ocs-storagecluster-ceph-rbd</code> <strong>StorageClass</strong>. This section walks you through
the steps to perform a PVC expansion.</p>
</div>
<div class="paragraph">
<p>We will first artificially fill up the PVC used by the application you have
just created.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc rsh -n my-database-app $(oc get pods -n my-database-app|grep postgresql | grep -v deploy | awk {'print $1}')</code></pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">df</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>Filesystem                           1K-blocks     Used Available Use% Mounted on
overlay                              125277164 12004092 113273072  10% /
tmpfs                                    65536        0     65536   0% /dev
tmpfs                                 32571336        0  32571336   0% /sys/fs/cgroup
shm                                      65536        8     65528   1% /dev/shm
tmpfs                                 32571336    10444  32560892   1% /etc/passwd
/dev/mapper/coreos-luks-root-nocrypt 125277164 12004092 113273072  10% /etc/hosts
/dev/rbd1                              5095040    66968   5011688   2% /var/lib/pgsql/data
tmpfs                                 32571336       28  32571308   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                                 32571336        0  32571336   0% /proc/acpi
tmpfs                                 32571336        0  32571336   0% /proc/scsi
tmpfs                                 32571336        0  32571336   0% /sys/firmware</pre>
</div>
</div>
<div class="paragraph">
<p>As observed in the output above the device named <code>/dev/rbd1</code>
is mounted as <code>/var/lib/pgsql/data</code>. This is the directory we will artificially
fill up.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">dd if=/dev/zero of=/var/lib/pgsql/data/fill.up bs=1M count=3850</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>3850+0 records in
3850+0 records out
4037017600 bytes (4.0 GB) copied, 13.6446 s, 296 MB/s</pre>
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s verify the volume mounted has increased.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">df</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>Filesystem                           1K-blocks     Used Available Use% Mounted on
overlay                              125277164 12028616 113248548  10% /
tmpfs                                    65536        0     65536   0% /dev
tmpfs                                 32571336        0  32571336   0% /sys/fs/cgroup
shm                                      65536        8     65528   1% /dev/shm
tmpfs                                 32571336    10444  32560892   1% /etc/passwd
/dev/mapper/coreos-luks-root-nocrypt 125277164 12028616 113248548  10% /etc/hosts
/dev/rbd1                              5095040  4009372   1069284  79% /var/lib/pgsql/data
tmpfs                                 32571336       28  32571308   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                                 32571336        0  32571336   0% /proc/acpi
tmpfs                                 32571336        0  32571336   0% /proc/scsi
tmpfs                                 32571336        0  32571336   0% /sys/firmware</pre>
</div>
</div>
<div class="paragraph">
<p>As observed in the output above, the filesystem usage for <code>/var/lib/pgsql/data</code>
has increased up to 79%. By default OCP will generate a PVC alert when a PVC
crosses the 75% full threshold.</p>
</div>
<div class="paragraph">
<p>Now exit the pod.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">exit</code></pre>
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s verify an alert has appeared in the OCP event log.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS-PVCResize-pvcnearfull-alert.png" alt="PVC nearfull alert">
</div>
<div class="title">Figure 15. OpenShift Container Platform Events</div>
</div>
<div class="sect3">
<h4 id="_expand_applying_a_modified_pvc_yaml_file"><a class="anchor" href="#_expand_applying_a_modified_pvc_yaml_file"></a>3.2.1. Expand applying a modified PVC YAML file</h4>
<div class="paragraph">
<p>To expand a <strong>PVC</strong> we simply need to change the actual amount of storage that is
requested. This can easily be performed by exporting the <strong>PVC</strong> specifications
into a YAML file with the following command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pvc postgresql -n my-database-app -o yaml &gt; pvc.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the file <code>pvc.yaml</code> that was created, search for the following section using
your favorite editor.</p>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">[truncated]
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: ocs-storagecluster-ceph-rbd
  volumeMode: Filesystem
  volumeName: pvc-4d6838df-b4cd-4bb1-9969-1af93c1dc5e6
status: {}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Edit <code>storage: 5Gi</code> and replace it with <code>storage: 10Gi</code>. The resulting section
in your file should look like the output below.</p>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">[truncated]
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: ocs-storagecluster-ceph-rbd
  volumeMode: Filesystem
  volumeName: pvc-4d6838df-b4cd-4bb1-9969-1af93c1dc5e6
status: {}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now you can apply your updated PVC specifications using the following command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc apply -f pvc.yaml -n my-database-app</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>Warning: oc apply should be used on resource created by either oc create
--save-config or oc apply persistentvolumeclaim/postgresql configured</pre>
</div>
</div>
<div class="paragraph">
<p>You can visualize the progress of the expansion of the PVC using the following
command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc describe pvc postgresql -n my-database-app</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>[truncated]
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      10Gi
Access Modes:  RWO
VolumeMode:    Filesystem
Mounted By:    postgresql-1-p62vw
Events:
  Type     Reason                      Age   From                                                                                                                Message
  ----     ------                      ----  ----                                                                                                                -------
  Normal   ExternalProvisioning        120m  persistentvolume-controller                                                                                         waiting for a volume to be created, either by external provisioner "openshift-storage.rbd.csi.ceph.com" or manually created by system administrator
  Normal   Provisioning                120m  openshift-storage.rbd.csi.ceph.com_csi-rbdplugin-provisioner-66f66699c8-gcm7t_3ce4b8bc-0894-4824-b23e-ed9bd46e7b41  External provisioner is provisioning volume for claim "my-database-app/postgresql"
  Normal   ProvisioningSucceeded       120m  openshift-storage.rbd.csi.ceph.com_csi-rbdplugin-provisioner-66f66699c8-gcm7t_3ce4b8bc-0894-4824-b23e-ed9bd46e7b41  Successfully provisioned volume pvc-4d6838df-b4cd-4bb1-9969-1af93c1dc5e6
  Warning  ExternalExpanding           65s   volume_expand                                                                                                       Ignoring the PVC: didn't find a plugin capable of expanding the volume; waiting for an external controller to process this PVC.
  Normal   Resizing                    65s   external-resizer openshift-storage.rbd.csi.ceph.com                                                                 External resizer is resizing volume pvc-4d6838df-b4cd-4bb1-9969-1af93c1dc5e6
  Normal   FileSystemResizeRequired    65s   external-resizer openshift-storage.rbd.csi.ceph.com                                                                 Require file system resize of volume on node
  Normal   FileSystemResizeSuccessful  23s   kubelet, ip-10-0-199-224.us-east-2.compute.internal                                                                 MountVolume.NodeExpandVolume succeeded for volume "pvc-4d6838df-b4cd-4bb1-9969-1af93c1dc5e6"</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The expansion process commonly takes over 30 seconds to complete and is
based on the workload of your pod. This is due to the fact that the expansion
requires the resizing of the underlying RBD image (pretty fast) while also
requiring the resize of the filesystem that sits on top of the block device. To
perform the latter the filesystem must be quiesced to be safely expanded.
</td>
</tr>
</table>
</div>
<div class="admonitionblock caution">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
Reducing the size of a <strong>PVC</strong> is NOT supported.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Another way to check on the expansion of the <strong>PVC</strong> is to simply display the
<strong>PVC</strong> information using the following command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pvc -n my-database-app</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
postgresql   Bound    pvc-4d6838df-b4cd-4bb1-9969-1af93c1dc5e6   10Gi       RWO            ocs-storagecluster-ceph-rbd   121m</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The <code>CAPACITY</code> column will reflect the new requested size when the
expansion process is complete.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Another method to check on the expansion of the <strong>PVC</strong> is to go through two
specific fields of the PVC object via the CLI.</p>
</div>
<div class="paragraph">
<p>The current allocated size for the <strong>PVC</strong> can be checked this way:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">echo $(oc get pvc postgresql -n my-database-app -o jsonpath='{.status.capacity.storage}')</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>10Gi</pre>
</div>
</div>
<div class="paragraph">
<p>The requested size for the <strong>PVC</strong> can be checked this way:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">echo $(oc get pvc postgresql -n my-database-app -o jsonpath='{.spec.resources.requests.storage}')</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>10Gi</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
When both results report the same value, the expansion was successful.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_expand_via_the_user_interface"><a class="anchor" href="#_expand_via_the_user_interface"></a>3.2.2. Expand via the User Interface</h4>
<div class="paragraph">
<p>The last method available to expand a <strong>PVC</strong> is to do so through the <strong>OpenShift
Web Console</strong>. Proceed as follow:</p>
</div>
<div class="paragraph">
<p>First step is to select the project to which the <strong>PVC</strong> belongs to.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS-PVCResize-select-project.png" alt="Select project">
</div>
<div class="title">Figure 16. Select the appropriate project</div>
</div>
<div class="paragraph">
<p>Choose <code>Expand PVC</code> from the contextual menu.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS-PVCResize-choose-expand-menu.png" alt="Choose expand from the contextual menu">
</div>
<div class="title">Figure 17. Choose Expand from menu</div>
</div>
<div class="paragraph">
<p>In the dialog box that appears enter the new capacity for the <strong>PVC</strong>.</p>
</div>
<div class="admonitionblock caution">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
You can NOT reduce the size of a <strong>PVC</strong>.
</td>
</tr>
</table>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS-PVCResize-enter-new-size.png" alt="Enter new size">
</div>
<div class="title">Figure 18. Enter the new size for the <strong>PVC</strong></div>
</div>
<div class="paragraph">
<p>You now simply have to wait for the expansion to complete and for the new size
to be reflected in the console (15 GiB).</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS-PVCResize-verify-resize-worked2.png" alt="Wait for expansion">
</div>
<div class="title">Figure 19. Wait for the expansion to complete</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_create_a_new_ocp_application_deployment_using_cephfs_volume"><a class="anchor" href="#_create_a_new_ocp_application_deployment_using_cephfs_volume"></a>4. Create a new OCP application deployment using CephFS volume</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this section the <code>ocs-storagecluster-cephfs</code> <strong>StorageClass</strong> will be used to
create a RWX (ReadWriteMany) <strong>PVC</strong> that can be used by multiple pods at the
same time. The application we will use is called <code>File Uploader</code>.</p>
</div>
<div class="paragraph">
<p>Create a new project:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc new-project my-shared-storage</code></pre>
</div>
</div>
<div class="paragraph">
<p>Next deploy the example PHP application called <code>file-uploader</code>:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc new-app openshift/php:7.2-ubi8~https://github.com/christianh814/openshift-php-upload-demo --name=file-uploader</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Sample Output:</div>
<div class="content">
<pre>--&gt; Found image 4f2dcc0 (9 days old) in image stream "openshift/php" under tag "7.2-ubi8" for "openshift/php:7.2-
ubi8"

    Apache 2.4 with PHP 7.2
    -----------------------
    PHP 7.2 available as container is a base platform for building and running various PHP 7.2 applications and f
rameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynam
ically generated web pages. PHP also offers built-in database integration for several commercial and non-commerci
al database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common
use of PHP coding is probably as a replacement for CGI scripts.

    Tags: builder, php, php72, php-72

    * A source build using source code from https://github.com/christianh814/openshift-php-upload-demo will be cr
eated
      * The resulting image will be pushed to image stream tag "file-uploader:latest"
      * Use 'oc start-build' to trigger a new build

--&gt; Creating resources ...
    imagestream.image.openshift.io "file-uploader" created
    buildconfig.build.openshift.io "file-uploader" created
    deployment.apps "file-uploader" created
    service "file-uploader" created
--&gt; Success
    Build scheduled, use 'oc logs -f buildconfig/file-uploader' to track its progress.
    Application is not exposed. You can expose services to the outside world by executing one or more of the comm
ands below:
     'oc expose service/file-uploader'
    Run 'oc status' to view your app.</pre>
</div>
</div>
<div class="paragraph">
<p>Watch the build log and wait for the application to be deployed:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc logs -f bc/file-uploader -n my-shared-storage</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example Output:</div>
<div class="content">
<pre>Cloning "https://github.com/christianh814/openshift-php-upload-demo" ...

[...]

Generating dockerfile with builder image image-registry.openshift-image-regis
try.svc:5000/openshift/php@sha256:d97466f33999951739a76bce922ab17088885db610c
0e05b593844b41d5494ea
STEP 1: FROM image-registry.openshift-image-registry.svc:5000/openshift/php@s
ha256:d97466f33999951739a76bce922ab17088885db610c0e05b593844b41d5494ea
STEP 2: LABEL "io.openshift.build.commit.author"="Christian Hernandez &lt;christ
ian.hernandez@yahoo.com&gt;"       "io.openshift.build.commit.date"="Sun Oct 1 1
7:15:09 2017 -0700"       "io.openshift.build.commit.id"="288eda3dff43b02f7f7
b6b6b6f93396ffdf34cb2"       "io.openshift.build.commit.ref"="master"       "
io.openshift.build.commit.message"="trying to modularize"       "io.openshift
.build.source-location"="https://github.com/christianh814/openshift-php-uploa
d-demo"       "io.openshift.build.image"="image-registry.openshift-image-regi
stry.svc:5000/openshift/php@sha256:d97466f33999951739a76bce922ab17088885db610
c0e05b593844b41d5494ea"
STEP 3: ENV OPENSHIFT_BUILD_NAME="file-uploader-1"     OPENSHIFT_BUILD_NAMESP
ACE="my-shared-storage"     OPENSHIFT_BUILD_SOURCE="https://github.com/christ
ianh814/openshift-php-upload-demo"     OPENSHIFT_BUILD_COMMIT="288eda3dff43b0
2f7f7b6b6b6f93396ffdf34cb2"
STEP 4: USER root
STEP 5: COPY upload/src /tmp/src
STEP 6: RUN chown -R 1001:0 /tmp/src
STEP 7: USER 1001
STEP 8: RUN /usr/libexec/s2i/assemble
---&gt; Installing application source...
=&gt; sourcing 20-copy-config.sh ...
---&gt; 17:24:39     Processing additional arbitrary httpd configuration provide
d by s2i ...
=&gt; sourcing 00-documentroot.conf ...
=&gt; sourcing 50-mpm-tuning.conf ...
=&gt; sourcing 40-ssl-certs.sh ...
STEP 9: CMD /usr/libexec/s2i/run
STEP 10: COMMIT temp.builder.openshift.io/my-shared-storage/file-uploader-1:3
b83e447
Getting image source signatures

[...]

Writing manifest to image destination
Storing signatures
Successfully pushed image-registry.openshift-image-registry.svc:5000/my-share
d-storage/file-uploader@sha256:929c0ce3dcc65a6f6e8bd44069862858db651358b88065
fb483d51f5d704e501
Push successful</pre>
</div>
</div>
<div class="paragraph">
<p>The command prompt returns out of the tail mode once you see <em>Push successful</em>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This use of the <code>new-app</code> command directly asked for application code to
be built and did not involve a template. That is why it only created a <strong>single
Pod</strong> deployment with a <strong>Service</strong> and no <strong>Route</strong>.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Let&#8217;s make our application production ready by exposing it via a <code>Route</code> and
scale to 3 instances for high availability:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc expose svc/file-uploader -n my-shared-storage</code></pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc scale --replicas=3 deploy/file-uploader -n my-shared-storage</code></pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pods -n my-shared-storage</code></pre>
</div>
</div>
<div class="paragraph">
<p>You should have 3 <code>file-uploader</code> <strong>Pods</strong> in a few minutes. Repeat the command above
until there are 3 <code>file-uploader</code> <strong>Pods</strong> in <code>Running</code> STATUS.</p>
</div>
<div class="admonitionblock caution">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Never attempt to store persistent data in a <strong>Pod</strong> that has no persistent
volume associated with it. <strong>Pods</strong> and their containers are ephemeral by
definition, and any stored data will be lost as soon as the <strong>Pod</strong> terminates
for whatever reason.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>We can fix this by providing shared persistent storage to this application.</p>
</div>
<div class="paragraph">
<p>You can create a <strong>PersistentVolumeClaim</strong> and attach it into an application with
the <code>oc set volume</code> command. Execute the following</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc set volume deploy/file-uploader --add --name=my-shared-storage \
-t pvc --claim-mode=ReadWriteMany --claim-size=1Gi \
--claim-name=my-shared-storage --claim-class=ocs-storagecluster-cephfs \
--mount-path=/opt/app-root/src/uploaded \
-n my-shared-storage</code></pre>
</div>
</div>
<div class="paragraph">
<p>This command will:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>create a <strong>PersistentVolumeClaim</strong></p>
</li>
<li>
<p>update the <strong>Deployment</strong> to include a <code>volume</code> definition</p>
</li>
<li>
<p>update the <strong>Deployment</strong> to attach a <code>volumemount</code> into the specified
<code>mount-path</code></p>
</li>
<li>
<p>cause a new deployment of the 3 application <strong>Pods</strong></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For more information on what <code>oc set volume</code> is capable of, look at its help
output with <code>oc set volume -h</code>. Now, let&#8217;s look at the result of adding the
volume:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pvc -n my-shared-storage</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Sample Output:</div>
<div class="content">
<pre>NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                AGE
my-shared-storage   Bound    pvc-c34bb9db-43a7-4eca-bc94-0251d7128721   1Gi        RWX            ocs-storagecluster-cephfs   47s</pre>
</div>
</div>
<div class="paragraph">
<p>Notice the <code>ACCESSMODE</code> being set to <strong>RWX</strong> (short for <code>ReadWriteMany</code>).</p>
</div>
<div class="paragraph">
<p>All 3 <code>file-uploader</code><strong>Pods</strong> are using the same <strong>RWX</strong> volume. Without this
<code>ACCESSMODE</code>, OpenShift will not attempt to attach multiple <strong>Pods</strong> to the
same <strong>PersistentVolume</strong> reliably. If you attempt to scale up deployments that
are using <strong>RWO</strong> or <code>ReadWriteOnce</code> storage, the <strong>Pods</strong> will actually all
become co-located on the same node.</p>
</div>
<div class="paragraph">
<p>Now let&#8217;s use the file uploader web application using your browser to upload
new files.</p>
</div>
<div class="paragraph">
<p>First, find the <strong>Route</strong> that has been created:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get route file-uploader -n my-shared-storage -o jsonpath --template="http://{.spec.host}{'\n'}"</code></pre>
</div>
</div>
<div class="paragraph">
<p>This will return a route similar to this one.</p>
</div>
<div class="listingblock">
<div class="title">Sample Output:</div>
<div class="content">
<pre>http://file-uploader-my-shared-storage.apps.cluster-ocs4-abdf.ocs4-abdf.sandbox744.opentlc.com</pre>
</div>
</div>
<div class="paragraph">
<p>Point your browser to the web application using your route above. <strong>Your <code>route</code>
will be different.</strong></p>
</div>
<div class="paragraph">
<p>The web app simply lists all uploaded files and offers the ability to upload
new ones as well as download the existing data. Right now there is
nothing.</p>
</div>
<div class="paragraph">
<p>Select an arbitrary file from your local machine and upload it to the app.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/uploader_screen_upload.png" alt="uploader screen upload">
</div>
<div class="title">Figure 20. A simple PHP-based file upload tool</div>
</div>
<div class="paragraph">
<p>Once done click <strong><em>List uploaded files</em></strong> to see the list of all currently
uploaded files.</p>
</div>
<div class="sect2">
<h3 id="_expand_cephfs_based_pvcs"><a class="anchor" href="#_expand_cephfs_based_pvcs"></a>4.1. Expand CephFS based PVCs</h3>
<div class="paragraph">
<p>OpenShift 4.5 and later versions let you expand an existing <strong>PVC</strong> based on the
<code>ocs-storagecluster-cephfs</code> <strong>StorageClass</strong>. This chapter walks you through the
steps to perform a PVC expansion through the CLI.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
All the other methods described for expanding a Ceph RBD based <strong>PVC</strong> are
also available.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The <code>my-sharged-storage</code> <strong>PVC</strong> size is currently <code>1Gi</code>. Let&#8217;s increase the size to <code>5Gi</code> using the <strong>oc patch</strong> command.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc patch pvc my-shared-storage -n my-shared-storage --type json --patch  '[{ "op": "replace", "path": "/spec/resources/requests/storage", "value": "5Gi" }]'</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>persistentvolumeclaim/my-shared-storage patched</pre>
</div>
</div>
<div class="paragraph">
<p>Now let&#8217;s verify the RWX <strong>PVC</strong> has been expanded.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">echo $(oc get pvc my-shared-storage -n my-shared-storage -o jsonpath='{.spec.resources.requests.storage}')</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>5Gi</pre>
</div>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">echo $(oc get pvc my-shared-storage -n my-shared-storage -o jsonpath='{.status.capacity.storage}')</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>5Gi</pre>
</div>
</div>
<div class="paragraph">
<p>Repeat both commands until output values are identical.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
CephFS based RWX <strong>PVC</strong> resizing, as opposed to RBD based <strong>PVCs</strong>, is
almost instantaneous. This is due to the fact that resizing such PVC does not
involved resizing a filesystem but simply involves updating a quota for the
mounted filesystem.
</td>
</tr>
</table>
</div>
<div class="admonitionblock caution">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
Reducing the size of a CephFS <strong>PVC</strong> is NOT supported.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_pvc_clone_and_snapshot"><a class="anchor" href="#_pvc_clone_and_snapshot"></a>5. PVC Clone and Snapshot</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Starting with version OCS version 4.6, the <code>Container Storage Interface</code> (CSI) features of being able to clone or snapshot a persistent volume are now supported. These new capabilities are very important for protecting persistent data and can be used with third party <code>Backup and Restore</code> vendors that have CSI integration.</p>
</div>
<div class="paragraph">
<p>In addition to third party backup and restore vendors, OCS snapshot for Ceph RBD and CephFS PVCs can be triggered using <code>OpenShift APIs for Data Protection</code> (OADP) which is an un-supported community operator in <strong>OperatorHub</strong> that can be very useful for testing backup and restore of persistent data including OpenShift metadata (definition files for pods, service, routes, deployments, etc.).</p>
</div>
<div class="sect2">
<h3 id="_pvc_clone"><a class="anchor" href="#_pvc_clone"></a>5.1. PVC Clone</h3>
<div class="paragraph">
<p>A CSI volume clone is a duplicate of an existing persistent volume at a particular point in time. Cloning creates an exact duplicate of the specified volume in OCS. After dynamic provisioning, you can use a volume clone just as you would use any standard volume.</p>
</div>
<div class="sect3">
<h4 id="_provisioning_a_csi_volume_clone"><a class="anchor" href="#_provisioning_a_csi_volume_clone"></a>5.1.1. Provisioning a CSI Volume clone</h4>
<div class="paragraph">
<p>For this exercise we will use the already created <strong>PVC</strong> <code>postgresql</code> that was just expanded to 15 GiB. Make sure you have done section <a href="#_create_a_new_ocp_application_deployment_using_ceph_rbd_volume">Create a new OCP application deployment using Ceph RBD volume</a> before proceeding.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pvc -n my-database-app | awk {'print $1}'</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME
postgresql</pre>
</div>
</div>
<div class="admonitionblock caution">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
Make sure you expanded the <code>postgresql</code> <strong>PVC</strong> to 15Gi before proceeding. If not expanded go back and complete this section <a href="#_expand_rbd_based_pvcs">Expand RBD based PVCs</a>.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Before creating the PVC clone make sure to create and save at least one new article so there is new data in the <code>postgresql</code> <strong>PVC</strong>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get route rails-pgsql-persistent -n my-database-app -o jsonpath --template="http://{.spec.host}/articles{'\n'}"</code></pre>
</div>
</div>
<div class="paragraph">
<p>This will return a route similar to this one.</p>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>http://rails-pgsql-persistent-my-database-app.apps.cluster-ocs4-8613.ocs4-8613.sandbox944.opentlc.com/articles</pre>
</div>
</div>
<div class="paragraph">
<p>Copy your route (different than above) to a browser window to create articles.</p>
</div>
<div class="paragraph">
<p>Enter the <code>username</code> and <code>password</code> below to create a new article.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>username: openshift
password: secret</pre>
</div>
</div>
<div class="paragraph">
<p>To protect the data (articles) in this <strong>PVC</strong> we will now clone this PVC. The operation of creating a clone can be done using the <strong>OpenShift Web Console</strong> or by creating the resource via a YAML file.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgresql-clone
  namespace: my-database-app
spec:
  storageClassName: ocs-storagecluster-ceph-rbd
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 15Gi
  dataSource:
    kind: PersistentVolumeClaim
    name: postgresql</code></pre>
</div>
</div>
<div class="paragraph">
<p>Doing the same operation in the <strong>OpenShift Web Console</strong> would require navigating to <code>Storage</code> &#8594; <code>Persistent Volume Claim</code> and choosing <code>Clone PVC</code>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCP4-OCS4-Clone-PVC.png" alt="Persistent Volume Claim clone PVC using UI">
</div>
<div class="title">Figure 21. Persistent Volume Claim clone PVC using UI</div>
</div>
<div class="paragraph">
<p>Size of new clone <strong>PVC</strong> is greyed out. The new <strong>PVC</strong> will be the same size as the original.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCP4-OCS4-Clone-PVC-config.png" alt="Persistent Volume Claim clone configuration">
</div>
<div class="title">Figure 22. Persistent Volume Claim clone configuration</div>
</div>
<div class="paragraph">
<p>Now create a <strong>PVC</strong> clone for <code>postgresql</code>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/training/modules/ocs4/attachments/postgresql-clone.yaml | oc apply -f -</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>persistentvolumeclaim/postgresql-clone created</pre>
</div>
</div>
<div class="paragraph">
<p>Now check to see there is a new <strong>PVC</strong>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pvc -n my-database-app | grep clone</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>postgresql-clone   Bound    pvc-f5e09c63-e8aa-48a0-99df-741280d35e42   15Gi       RWO            ocs-storagecluster-ceph-rbd   3m47s</pre>
</div>
</div>
<div class="paragraph">
<p>You can also check the new clone <strong>PVC</strong> in the <strong>OpenShift Web Console</strong>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCP4-OCS4-Clone-PVC-view.png" alt="Persistent Volume Claim clone view in UI">
</div>
<div class="title">Figure 23. Persistent Volume Claim clone view in UI</div>
</div>
</div>
<div class="sect3">
<h4 id="_using_a_csi_volume_clone_for_application_recovery"><a class="anchor" href="#_using_a_csi_volume_clone_for_application_recovery"></a>5.1.2. Using a CSI Volume clone for application recovery</h4>
<div class="paragraph">
<p>Now that you have a clone for <code>postgresql</code> <strong>PVC</strong> you are ready to test by corrupting the database.</p>
</div>
<div class="paragraph">
<p>The following command will print all <code>postgresql</code> tables before deleting the article tables in the database and after the tables are deleted.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc rsh -n my-database-app $(oc get pods -n my-database-app|grep postgresql | grep -v deploy | awk {'print $1}') psql -c "\c root" -c "\d+" -c "drop table articles cascade;" -c "\d+"</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>You are now connected to database "root" as user "postgres".
                               List of relations
 Schema |         Name         |   Type   |  Owner  |    Size    | Description
--------+----------------------+----------+---------+------------+-------------
 public | ar_internal_metadata | table    | userXNL | 16 kB      |
 public | articles             | table    | userXNL | 16 kB      |
 public | articles_id_seq      | sequence | userXNL | 8192 bytes |
 public | comments             | table    | userXNL | 8192 bytes |
 public | comments_id_seq      | sequence | userXNL | 8192 bytes |
 public | schema_migrations    | table    | userXNL | 16 kB      |
(6 rows)

NOTICE:  drop cascades to constraint fk_rails_3bf61a60d3 on table comments
DROP TABLE
                               List of relations
 Schema |         Name         |   Type   |  Owner  |    Size    | Description
--------+----------------------+----------+---------+------------+-------------
 public | ar_internal_metadata | table    | userXNL | 16 kB      |
 public | comments             | table    | userXNL | 8192 bytes |
 public | comments_id_seq      | sequence | userXNL | 8192 bytes |
 public | schema_migrations    | table    | userXNL | 16 kB      |
(4 rows)</pre>
</div>
</div>
<div class="paragraph">
<p>Now go back to the browser tab where you created your article using this link:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get route rails-pgsql-persistent -n my-database-app -o jsonpath --template="http://{.spec.host}/articles{'\n'}"</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you refresh the browser you will see the application has failed.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/rails-postgresql-failed.png" alt="Application failed because database table removed">
</div>
<div class="title">Figure 24. Application failed because database table removed</div>
</div>
<div class="paragraph">
<p>Remember a <strong>PVC</strong> clone is an exact duplica of the original <strong>PVC</strong> at the time the clone was created. Therefore you can use you <code>postgresql</code> clone to recover the application.</p>
</div>
<div class="paragraph">
<p>First you need to scale the <code>rails-pgsql-persistent</code> deployment down to zero so the <strong>Pod</strong> will be deleted.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc scale deploymentconfig rails-pgsql-persistent -n my-database-app --replicas=0</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>deploymentconfig.apps.openshift.io/rails-pgsql-persistent scaled</pre>
</div>
</div>
<div class="paragraph">
<p>Verify the <strong>Pod</strong> is gone.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pods -n my-database-app | grep rails | egrep -v 'deploy|build|hook' | awk {'print $1}'</code></pre>
</div>
</div>
<div class="paragraph">
<p>Wait until there is no result for this command. Repeat if necessary.</p>
</div>
<div class="paragraph">
<p>Now you need to patch the deployment for <code>postgesql</code> and modify to use the <code>postgresql-clone</code> <strong>PVC</strong>. This can be done using the <code>oc patch</code> command.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc patch dc postgresql -n my-database-app --type json --patch  '[{ "op": "replace", "path": "/spec/template/spec/volumes/0/persistentVolumeClaim/claimName", "value": "postgresql-clone" }]'</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>deploymentconfig.apps.openshift.io/postgresql patched</pre>
</div>
</div>
<div class="paragraph">
<p>After modifying the deployment with the clone <strong>PVC</strong> the <code>rails-pgsql-persistent</code> deployment needs to be scaled back up.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc scale deploymentconfig rails-pgsql-persistent -n my-database-app --replicas=1</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>deploymentconfig.apps.openshift.io/rails-pgsql-persistent scaled</pre>
</div>
</div>
<div class="paragraph">
<p>Now check to see that there is a new <code>postgresql</code> and <code>rails-pgsql-persistent</code> <strong>Pod</strong>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pods -n my-database-app | egrep 'rails|postgresql' | egrep -v 'deploy|build|hook'</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>postgresql-4-hv5kb                  1/1     Running     0          5m58s
rails-pgsql-persistent-1-dhwhz      1/1     Running     0          5m10s</pre>
</div>
</div>
<div class="paragraph">
<p>Go back to the browser tab where you created your article using this link:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get route rails-pgsql-persistent -n my-database-app -o jsonpath --template="http://{.spec.host}/articles{'\n'}"</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you refresh the browser you will see the application is back online and you have your articles. You can even add more articles now.</p>
</div>
<div class="paragraph">
<p>This process shows the pratical reasons to create a <strong>PVC</strong> clone if you are testing an application where data corruption is a possibility and you want a known good copy or <code>clone</code>.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s next look at a similar feature, creating a <strong>PVC</strong> snapshot.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_pvc_snapshot"><a class="anchor" href="#_pvc_snapshot"></a>5.2. PVC Snapshot</h3>
<div class="paragraph">
<p>Creating the first snapshot of a PVC is the same as creating a clone from that PVC. However, after an initial PVC snapshot is created, subsequent snapshots only save the delta between the initial snapshot the current contents of the PVC. Snapshots are frequently used by backup utilities which schedule incremental backups on a periodic basis (e.g. hourly). Snapshots are more capacity efficient than creating full clones each time period (e.g. hourly), as only the deltas to the PVC are stored in each snapshot.</p>
</div>
<div class="paragraph">
<p>A snapshot can be used to provision a new volume by creating a <strong>PVC</strong> clone. The volume clone can be used for application recovery as demonstrated in the previous section.</p>
</div>
<div class="sect3">
<h4 id="_volumesnapshotclass"><a class="anchor" href="#_volumesnapshotclass"></a>5.2.1. VolumeSnapshotClass</h4>
<div class="paragraph">
<p>To create a volume snapshot there first must be <strong>VolumeSnapshotClass</strong> resources that will be referenced in the <strong>VolumeSnapshot</strong> definition. The deployment of OCS (must be version 4.6 or greater) creates two <strong>VolumeSnapshotClass</strong> resources for creating snapshots.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get volumesnapshotclasses</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>$ oc get volumesnapshotclasses
NAME                                        DRIVER                                  DELETIONPOLICY   AGE
ocs-storagecluster-cephfsplugin-snapclass   openshift-storage.cephfs.csi.ceph.com   Delete           4d23h
ocs-storagecluster-rbdplugin-snapclass      openshift-storage.rbd.csi.ceph.com      Delete           4d23h</pre>
</div>
</div>
<div class="paragraph">
<p>You can see by the naming of the <strong>VolumeSnapshotClass</strong> that one is for creating CephFS volume snapshots and the other is for Ceph RBD.</p>
</div>
</div>
<div class="sect3">
<h4 id="_provisioning_a_csi_volume_snapshot"><a class="anchor" href="#_provisioning_a_csi_volume_snapshot"></a>5.2.2. Provisioning a CSI Volume snapshot</h4>
<div class="paragraph">
<p>For this exercise we will use the already created <strong>PVC</strong> <code>my-shared-storage</code>. Make sure you have done section <a href="#_create_a_new_ocp_application_deployment_using_cephfs_volume">Create a new OCP application deployment using CephFS volume</a> before proceeding.</p>
</div>
<div class="paragraph">
<p>The operation of creating a snapshot can be done using the <strong>OpenShift Web Console</strong> or by creating the resource via a YAML file.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshot
metadata:
  name: my-shared-storage-snapshot
  namespace: my-shared-storage
spec:
  volumeSnapshotClassName: ocs-storagecluster-cephfsplugin-snapclass
  source:
    persistentVolumeClaimName: my-shared-storage</code></pre>
</div>
</div>
<div class="paragraph">
<p>Doing the same operation in the <strong>OpenShift Web Console</strong> would require navigating to <code>Storage</code> &#8594; <code>Persistent Volume Claim</code> and choosing <code>Create Snapshot</code>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCP4-OCS4-Snapshot.png" alt="Persistent Volume Claim snapshot using UI">
</div>
<div class="title">Figure 25. Persistent Volume Claim snapshot using UI</div>
</div>
<div class="paragraph">
<p>Now create a snapshot for CephFS volume <code>my-shared-storage</code>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/training/modules/ocs4/attachments/my-shared-storage-snapshot.yaml | oc apply -f -</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>volumesnapshot.snapshot.storage.k8s.io/my-shared-storage-snapshot created</pre>
</div>
</div>
<div class="paragraph">
<p>Now check to see there is a new <strong>VolumeSnapshot</strong>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get volumesnapshot -n my-shared-storage</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                         READYTOUSE   SOURCEPVC           SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS                               SNAPSHOTCONTENT                                   CREATIONTIME   AGE
my-shared-storage-snapshot   true         my-shared-storage                           5Gi           ocs-storagecluster-cephfsplugin-snapclass   snapcontent-2d4729bc-a127-4da6-930d-2a7d0125d3b7   24s            26s</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_restoring_volume_snapshot_to_clone_pvc"><a class="anchor" href="#_restoring_volume_snapshot_to_clone_pvc"></a>5.2.3. Restoring Volume Snapshot to clone PVC</h4>
<div class="paragraph">
<p>You can now restore the new <strong>VolumeSnapshot</strong> in the <strong>OpenShift Web Console</strong>. Navigate to <code>Storage</code> &#8594; <code>Volume Snapshots</code>. Select <code>Restore as new PVC</code>. Make sure to have the <code>my-shared-storage</code> project selected at the top left.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCP4-OCS4-Snapshot-restore.png" alt="Persistent Volume Claim snapshot restore in UI">
</div>
<div class="title">Figure 26. Persistent Volume Claim snapshot restore in UI</div>
</div>
<div class="paragraph">
<p>Chose the correct <strong>StorageClass</strong> to create the new clone from snapshot <strong>PVC</strong> and select <code>Restore</code>. The size of the new <strong>PVC</strong> is greyed out and is same as the <code>parent</code> or original <strong>PVC</strong> <code>my-shared-storage</code>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCP4-OCS4-Snapshot-restore-config.png" alt="Persistent Volume Claim snapshot restore configuration">
</div>
<div class="title">Figure 27. Persistent Volume Claim snapshot restore configuration</div>
</div>
<div class="paragraph">
<p>Check to see if there is a new <strong>PVC</strong> restored from the <strong>VolumeSnapshot</strong>.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pvc -n my-shared-storage | grep restore</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>my-shared-storage-snapshot-restore   Bound    pvc-24999d30-09f1-4142-b150-a5486df7b3f1   5Gi        RWX            ocs-storagecluster-cephfs   108s</pre>
</div>
</div>
<div class="paragraph">
<p>The output shows a new <strong>PVC</strong> that could be used to recover an application if there is corruption or lost data.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_using_ocs_for_prometheus_metrics"><a class="anchor" href="#_using_ocs_for_prometheus_metrics"></a>6. Using OCS for Prometheus Metrics</h2>
<div class="sectionbody">
<div class="paragraph">
<p>OpenShift ships with a pre-configured and self-updating monitoring stack that
is based on the Prometheus open source project and its wider eco-system. It
provides monitoring of cluster components and ships with a set of alerts to
immediately notify the cluster administrator about any occurring problems. For
production environments, it is highly recommended to configure persistent
storage using block storage technology. OCS 4 provide block storage using Ceph
RBD volumes. Running cluster monitoring with persistent storage means that your
metrics are stored to a persistent volume and can survive a pod being restarted
or recreated. This section will detail how to migrate Prometheus and
AlertManager storage to Ceph RBD volumes for persistence.</p>
</div>
<div class="paragraph">
<p>First, let&#8217;s discover what <strong>Pods</strong> and <strong>PVCs</strong> are installed in the
<code>openshift-monitoring</code> namespace. In the prior module, OpenShift Infrastructure
Nodes, the Prometheus and AlertManager resources were moved to the OCP infra
nodes.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pods,pvc -n openshift-monitoring</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                                               READY   STATUS         RESTARTS   AGE
pod/alertmanager-main-0                            5/5     Running        0          6d21h
pod/alertmanager-main-1                            5/5     Running        0          6d21h
pod/alertmanager-main-2                            5/5     Running        0          6d21h
pod/cluster-monitoring-operator-595888fddd-mcgnl   2/2     Running        0          4h49m
pod/grafana-65454464fd-5spx2                       2/2     Running        0          26h
pod/kube-state-metrics-7cb89d65d4-p9hbd            3/3     Running        0          6d21h
pod/node-exporter-96zjb                            2/2     Running        0          6d21h
pod/node-exporter-9jjdk                            2/2     Running        0          2d17h
pod/node-exporter-dhnt4                            2/2     Running        0          6d21h
pod/node-exporter-kg2fb                            2/2     Running        0          2d17h
pod/node-exporter-l27n2                            2/2     Running        0          16h
pod/node-exporter-qq4g7                            2/2     Running        0          16h
pod/node-exporter-rfnxb                            2/2     Running        0          16h
pod/node-exporter-v8kpq                            2/2     Running        0          2d17h
pod/node-exporter-wvm8n                            2/2     Running        0          6d21h
pod/node-exporter-wwcr9                            2/2     Running        0          6d21h
pod/node-exporter-z8r98                            2/2     Running        0          6d21h
pod/openshift-state-metrics-57969c7f87-h8fm4       3/3     Running        0          6d21h
pod/prometheus-adapter-cb658c44-zmcww              1/1     Running        0          2d22h
pod/prometheus-adapter-cb658c44-zsn85              1/1     Running        0          2d22h
pod/prometheus-k8s-0                               6/6     Running        0          6d21h
pod/prometheus-k8s-1                               6/6     Running        0          6d21h
pod/prometheus-operator-8594bd77df-ftwvl           2/2     Running        0          26h
pod/telemeter-client-79d7ddbf84-ft97l              3/3     Running        0          42h
pod/thanos-querier-787547fbd6-qw9tr                5/5     Running        0          6d21h
pod/thanos-querier-787547fbd6-xdsmm                5/5     Running        0          6d21h</pre>
</div>
</div>
<div class="paragraph">
<p>At this point there are no <strong>PVC</strong> resources because Prometheus and AlertManager
are both using ephemeral (EmptyDir) storage. This is the way OpenShift is
initially installed. The Prometheus stack consists of the Prometheus database
and the alertmanager data. Persisting both is best-practice since data loss on
either of these will cause you to lose your metrics and alerting data.</p>
</div>
<div class="sect2">
<h3 id="_modifying_your_prometheus_environment"><a class="anchor" href="#_modifying_your_prometheus_environment"></a>6.1. Modifying your Prometheus environment</h3>
<div class="paragraph">
<p>For Prometheus every supported configuration change is controlled through a
central <strong>ConfigMap</strong>, which needs to exist before we can make changes. When you
start off with a clean installation of Openshift, the <strong>ConfigMap</strong> to configure
the Prometheus environment may not be present. To check if your <strong>ConfigMap</strong> is
present, execute this:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc -n openshift-monitoring get configmap cluster-monitoring-config</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Output if the <strong>ConfigMap</strong> is not yet created:</div>
<div class="content">
<pre>Error from server (NotFound): configmaps "cluster-monitoring-config" not found</pre>
</div>
</div>
<div class="listingblock">
<div class="title">Output if the <strong>ConfigMap</strong> is created:</div>
<div class="content">
<pre>NAME                        DATA   AGE
cluster-monitoring-config   1      116m</pre>
</div>
</div>
<div class="paragraph">
<p>If you are missing the <strong>ConfigMap</strong>, create it using this command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/training/modules/ocs4/attachments/cluster-monitoring-config.yaml | oc apply -f -</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>configmap/cluster-monitoring-config created</pre>
</div>
</div>
<div class="paragraph">
<p>You can view the <strong>ConfigMap</strong> with the following command:</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The size of the Ceph RBD volumes, <code>40Gi</code>, can be modified to be larger or
smaller depending on requirements.
</td>
</tr>
</table>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc -n openshift-monitoring get configmap cluster-monitoring-config -o yaml | more</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title"><strong>ConfigMap</strong> sample output:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">[...]
      volumeClaimTemplate:
        metadata:
          name: prometheusdb
        spec:
          storageClassName: ocs-storagecluster-ceph-rbd
          resources:
            requests:
              storage: 40Gi
[...]
      volumeClaimTemplate:
        metadata:
          name: alertmanager
        spec:
          storageClassName: ocs-storagecluster-ceph-rbd
          resources:
            requests:
              storage: 40Gi
[...]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once you create this new <strong>ConfigMap</strong> <code>cluster-monitoring-config</code>, the
affected <strong>Pods</strong> will automatically be restarted and the new storage will be
mounted in the Pods.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
It is not possible to retain data that was written on the default
EmptyDir-based or ephemeral installation. Thus you will start with an empty
DB after changing the backend storage thereby starting over with metric
collection and reporting.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>After a couple of minutes, the AlertManager and Prometheus <strong>Pods</strong> will have
restarted and you will see new <strong>PVCs</strong> in the <code>openshift-monitoring</code> namespace
that they are now providing persistent storage.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pods,pvc -n openshift-monitoring</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
[...]
alertmanager-alertmanager-main-0   Bound    pvc-733be285-aaf9-4334-9662-44b63bb4efdf   40Gi       RWO            ocs-storagecluster-ceph-rbd   3m37s
alertmanager-alertmanager-main-1   Bound    pvc-e07ebe61-de5d-404c-9a25-bb3a677281c5   40Gi       RWO            ocs-storagecluster-ceph-rbd   3m37s
alertmanager-alertmanager-main-2   Bound    pvc-9de2edf2-9f5e-4f62-8aa7-ecfd01957748   40Gi       RWO            ocs-storagecluster-ceph-rbd   3m37s
prometheusdb-prometheus-k8s-0      Bound    pvc-5b845908-d929-4326-976e-0659901468e9   40Gi       RWO            ocs-storagecluster-ceph-rbd   3m31s
prometheusdb-prometheus-k8s-1      Bound    pvc-f2d22176-6348-451f-9ede-c00b303339af   40Gi       RWO            ocs-storagecluster-ceph-rbd   3m31s</pre>
</div>
</div>
<div class="paragraph">
<p>You can validate that Prometheus and AlertManager are working correctly after
moving to persistent storage <a href="#_monitoring_the_ocs_environment">Monitoring the OCS environment</a> in a later
section of this lab guide.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_create_a_new_ocp_application_deployment_using_an_object_bucket"><a class="anchor" href="#_create_a_new_ocp_application_deployment_using_an_object_bucket"></a>7. Create a new OCP application deployment using an Object Bucket</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this section, you will deploy a new OCP application that uses <code>Object Bucket
Claims</code> (OBCs) to create dynamic buckets via the <code>Multicloud Object Gateway</code>
(MCG). You will also use the <code>MCG Console</code> to validate new objects in the
<code>Object Bucket</code>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The <code>MCG Console</code> is not fully integrated with the <strong>Openshift Web Console</strong>
and resources created in the <code>MCG Console</code> are not synchronized back to the
Openshift Cluster. For MCG features such as Namespace buckets, please use the
MCG console to configure.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_checking_mcg_status"><a class="anchor" href="#_checking_mcg_status"></a>7.1. Checking MCG status</h3>
<div class="paragraph">
<p>MCG status can be checked with the NooBaa CLI. You may download the NooBaa CLI
from the NooBaa Operator releases page: <a href="https://github.com/noobaa/noobaa-operator/releases" class="bare">https://github.com/noobaa/noobaa-operator/releases</a>. There is also instructions at <a href="#_install_the_noobaa_cli_client">Install the NooBaa CLI client</a>.</p>
</div>
<div class="paragraph">
<p>Make sure you are in the <code>openshift-storage</code> project when you execute this
command.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">noobaa status -n openshift-storage</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>INFO[0000] CLI version: 5.6.0
INFO[0000] noobaa-image: noobaa/noobaa-core:5.6.0
INFO[0000] operator-image: noobaa/noobaa-operator:5.6.0
INFO[0000] Namespace: openshift-storage
INFO[0000]
INFO[0000] CRD Status:
INFO[0000] ✅ Exists: CustomResourceDefinition "noobaas.noobaa.io"
INFO[0000] ✅ Exists: CustomResourceDefinition "backingstores.noobaa.io"
INFO[0000] ✅ Exists: CustomResourceDefinition "bucketclasses.noobaa.io"
INFO[0000] ✅ Exists: CustomResourceDefinition "objectbucketclaims.objectbucket.io"
INFO[0000] ✅ Exists: CustomResourceDefinition "objectbuckets.objectbucket.io"
INFO[0000]
INFO[0000] Operator Status:
INFO[0000] ✅ Exists: Namespace "openshift-storage"
INFO[0000] ✅ Exists: ServiceAccount "noobaa"
INFO[0000] ✅ Exists: Role "ocs-operator.v4.6.0-noobaa-6649766bf4"
INFO[0000] ✅ Exists: RoleBinding "ocs-operator.v4.6.0-noobaa-6649766bf4"
INFO[0000] ✅ Exists: ClusterRole "ocs-operator.v4.6.0-65577bfbc"
INFO[0000] ✅ Exists: ClusterRoleBinding "ocs-operator.v4.6.0-65577bfbc"
INFO[0000] ✅ Exists: Deployment "noobaa-operator"
INFO[0000]
INFO[0000] System Status:
INFO[0000] ✅ Exists: NooBaa "noobaa"
INFO[0000] ✅ Exists: StatefulSet "noobaa-core"
INFO[0000] ✅ Exists: Service "noobaa-mgmt"
INFO[0000] ✅ Exists: Service "s3"
INFO[0000] ✅ Exists: StatefulSet "noobaa-db"
INFO[0000] ✅ Exists: Service "noobaa-db"
INFO[0000] ✅ Exists: Secret "noobaa-server"
INFO[0000] ✅ Exists: Secret "noobaa-operator"
INFO[0000] ✅ Exists: Secret "noobaa-endpoints"
INFO[0000] ✅ Exists: Secret "noobaa-admin"
INFO[0000] ✅ Exists: StorageClass "openshift-storage.noobaa.io"
INFO[0000] ✅ Exists: BucketClass "noobaa-default-bucket-class"
INFO[0000] ✅ Exists: Deployment "noobaa-endpoint"
INFO[0000] ✅ Exists: HorizontalPodAutoscaler "noobaa-endpoint"
INFO[0000] ✅ (Optional) Exists: BackingStore "noobaa-default-backing-store"
INFO[0000] ✅ (Optional) Exists: CredentialsRequest "noobaa-aws-cloud-creds"
INFO[0000] ⬛ (Optional) Not Found: CredentialsRequest "noobaa-azure-cloud-creds"
INFO[0000] ⬛ (Optional) Not Found: Secret "noobaa-azure-container-creds"
INFO[0000] ⬛ (Optional) Not Found: Secret "noobaa-gcp-bucket-creds"
INFO[0000] ⬛ (Optional) Not Found: CredentialsRequest "noobaa-gcp-cloud-creds"
INFO[0000] ✅ (Optional) Exists: PrometheusRule "noobaa-prometheus-rules"
INFO[0000] ✅ (Optional) Exists: ServiceMonitor "noobaa-service-monitor"
INFO[0000] ✅ (Optional) Exists: Route "noobaa-mgmt"
INFO[0000] ✅ (Optional) Exists: Route "s3"
INFO[0000] ✅ Exists: PersistentVolumeClaim "db-noobaa-db-0"
INFO[0000] ✅ System Phase is "Ready"
INFO[0000] ✅ Exists:  "noobaa-admin"

#------------------#
#- Mgmt Addresses -#
#------------------#

ExternalDNS : [https://noobaa-mgmt-openshift-storage.apps.cluster-ocs4-8613.ocs4-8613.sandbox944.opentlc.com https://af3f0dd25ab0f4c7ba70f101f112ef0c-11
5712529.us-east-2.elb.amazonaws.com:443]
ExternalIP  : []
NodePorts   : [https://10.0.209.53:31759]
InternalDNS : [https://noobaa-mgmt.openshift-storage.svc:443]
InternalIP  : [https://172.30.22.156:443]
PodPorts    : [https://10.131.2.11:8443]

#--------------------#
#- Mgmt Credentials -#
#--------------------#

email    : admin@noobaa.io
password : Mph5Mhg/r2lCWj99O4jWjw==

#----------------#
#- S3 Addresses -#
#----------------#

ExternalDNS : [https://s3-openshift-storage.apps.cluster-ocs4-8613.ocs4-8613.sandbox944.opentlc.com https://a2087e1ee6e754d70bb96dd8922435b3-1451584877.
us-east-2.elb.amazonaws.com:443]
ExternalIP  : []
NodePorts   : [https://10.0.147.230:32297]
InternalDNS : [https://s3.openshift-storage.svc:443]
InternalIP  : [https://172.30.54.94:443]
PodPorts    : [https://10.130.2.70:6443]

#------------------#
#- S3 Credentials -#
#------------------#

AWS_ACCESS_KEY_ID     : SBC4HsLagqAy7IrGK2A3
AWS_SECRET_ACCESS_KEY : anilMy0atqj/QlVXMwNwbGasUpRJTXDM7/Mmt/AN

#------------------#
#- Backing Stores -#
#------------------#

NAME                           TYPE     TARGET-BUCKET                                       PHASE   AGE
noobaa-default-backing-store   aws-s3   nb.1610563076824.ocs4-8613.sandbox944.opentlc.com   Ready   7h9m5s

#------------------#
#- Bucket Classes -#
#------------------#

NAME                          PLACEMENT                                                             PHASE   AGE
noobaa-default-bucket-class   {Tiers:[{Placement: BackingStores:[noobaa-default-backing-store]}]}   Ready   7h9m5s

#-----------------#
#- Bucket Claims -#
#-----------------#

No OBCs found.</pre>
</div>
</div>
<div class="paragraph">
<p>The NooBaa status command will first check on the environment and will then
print all the information about the environment. Besides the status of the MCG,
the second most intersting information for us are the available S3 addresses
that we can use to connect to our MCG buckets. We can chose between using the
external DNS which incurs DNS traffic cost, or route internally inside of our
Openshift cluster.</p>
</div>
<div class="paragraph">
<p>You can get a more basic overview of the MCG status using the <code>Object Service</code>
<strong>Dashboard</strong>. To reach this, log into the <strong>Openshift Web Console</strong>, click on
<code>Home</code> and select the <code>Overview</code> item. In the main view, select <code>Object
Service</code> in the top navigation bar. This dashboard does not give you connection
information for your S3 endpoint, but offers Graphs and runtime information
about the usage of your S3 backend as well as a link to the <code>MCG Console</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_creating_and_using_object_bucket_claims"><a class="anchor" href="#_creating_and_using_object_bucket_claims"></a>7.2. Creating and Using Object Bucket Claims</h3>
<div class="paragraph">
<p>MCG <strong>ObjectBucketClaims</strong> (OBCs) are used to dynamically create S3 compatible
buckets that can be used by an OCP application. When an OBC is created MCG
creates a new <strong>ObjectBucket</strong> (OB), <strong>ConfigMap</strong> (CM) and <strong>Secret</strong> that together
contain all the information your application needs to connect to the new bucket
from within your deployment.</p>
</div>
<div class="paragraph">
<p>To demonstrate this feature we will use the Photo-Album demo application.</p>
</div>
<div class="paragraph">
<p>First download and extract the photo-album tarball.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">curl -L -s https://github.com/red-hat-storage/demo-apps/blob/main/packaged/photo-album.tgz?raw=true | tar xvz</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>photo-album/
photo-album/documentation/
photo-album/app/
photo-album/demo.sh
[...]</pre>
</div>
</div>
<div class="paragraph">
<p>Then, run the application startup script which will build and deploy the application to your cluster.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">cd photo-album
./demo.sh</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Please make sure you follow the continuation prompts by pressing enter.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>[ OK    ] Using apps.cluster-7c31.7c31.sandbox905.opentlc.com as our base domain

Object Bucket Demo

 * Cleanup existing environment

Press any key to continue...
[ OK    ] oc delete --ignore-not-found=1 -f app.yaml

[ OK    ] oc delete --ignore-not-found=1 bc photo-album -n demo
buildconfig.build.openshift.io "photo-album" deleted

 * Import dependencies and create build config
-./demo.sh
[ OK    ] Using apps.cluster-7c31.7c31.sandbox905.opentlc.com as our base domain

Object Bucket Demo

 * Cleanup existing environment

Press any key to continue...
[ OK    ] oc delete --ignore-not-found=1 -f app.yaml

[ OK    ] oc delete --ignore-not-found=1 bc photo-album -n demo
buildconfig.build.openshift.io "photo-album" deleted

 * Import dependencies and create build config

[...]
 OK    ] oc start-build photo-album --from-dir . -F -n demo
photo-album setup
/opt/app-root/src/demo-apps/photo-album</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<strong>Deployment might take up to 5 minutes or more to complete.</strong>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Check the photo-album deployment is complete by running:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc -n demo get pods</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                   READY   STATUS      RESTARTS   AGE
photo-album-1-build    0/1     Completed   0          10m
photo-album-1-deploy   0/1     Completed   0          10m
photo-album-1-rtplt    1/1     Running     0          10m</pre>
</div>
</div>
<div class="paragraph">
<p>Now that the photo-album application has been deployed you can view the <strong>ObjectBucketClaim</strong> it created. Run the following:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc -n demo get obc</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME          STORAGE-CLASS                 PHASE   AGE
photo-album   openshift-storage.noobaa.io   Bound   23m</pre>
</div>
</div>
<div class="paragraph">
<p>To view the <strong>ObjectBucket</strong> (OB) that was created by the <strong>OBC</strong> above run the following:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get ob</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>NAME                   STORAGE-CLASS                 CLAIM-NAMESPACE   CLAIM-NAME    RECLAIM-POLICY   PHASE   AGE
obc-demo-photo-album   openshift-storage.noobaa.io   demo              photo-album   Delete           Bound   23m</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<strong>OBs</strong>, similar to <strong>PVs</strong>, are cluster-scoped resources so therefore adding the namespace is not needed.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>You can also view the new bucket <strong>ConfigMap</strong> and <strong>Secret</strong> using the following commands.</p>
</div>
<div class="paragraph">
<p>The <strong>ConfigMap</strong> will contain important information such as the bucket name, service and port. All are used to configure the connection from within the deployment to the s3 endpoint.</p>
</div>
<div class="paragraph">
<p>To view the <strong>ConfigMap</strong> created by the OBC, run the following:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc -n demo get cm photo-album -o yaml | more</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: v1
data:
  BUCKET_HOST: s3.openshift-storage.svc
  BUCKET_NAME: photo-album-2c0d8504-ae02-4632-af83-b8b458b9b923
  BUCKET_PORT: "443"
  BUCKET_REGION: ""
  BUCKET_SUBREGION: ""
kind: ConfigMap
[...]</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <strong>Secret</strong> will contain the credentials required for the application to connect and access the new object bucket. The credentials or keys are <code>base64</code> encoded in the <strong>Secret</strong>.</p>
</div>
<div class="paragraph">
<p>To view the <strong>Secret</strong> created for the OBC run the following:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc -n demo get secret photo-album -o yaml | more</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: v1
data:
  AWS_ACCESS_KEY_ID: MTAyc3pJNnBsM3dXV0hOUzUyTEk=
  AWS_SECRET_ACCESS_KEY: cWpyWWhuendDcjNaR1ZyVkZVN1p4c2hRK2xicy9XVW1ETk50QmJpWg==
kind: Secret
[...]</code></pre>
</div>
</div>
<div class="paragraph">
<p>As you can see when the new <strong>OBC</strong> and <strong>OB</strong> are created, MCG creates an associated <strong>Secret</strong> and <strong>ConfigMap</strong> which contain all the information required for our photo-album application to use the new bucket.</p>
</div>
<div class="paragraph">
<p>In order to view the details of the <strong>ObjectBucketClaim</strong> view the start of <code>photo-album/app.yaml</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">---
apiVersion: objectbucket.io/v1alpha1
kind: ObjectBucketClaim
metadata:
  name: "photo-album"
  namespace: demo
spec:
  generateBucketName: "photo-album"
  storageClassName: openshift-storage.noobaa.io
---
[...]</code></pre>
</div>
</div>
<div class="paragraph">
<p>To view exactly how the application uses the information in the new <strong>Secret</strong> and <strong>ConfigMap</strong> have a look at the file <code>photo-album/app.yaml</code> after you have deployed the app. In the <strong>DeploymentConfig</strong> specification section, find <code>env:</code> and you can see how the <strong>ConfigMap</strong> and <strong>Secret</strong> details are mapped to environment variables.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">[...]
     spec:
        containers:
        - image: image-registry.openshift-image-registry.svc:5000/default/photo-album
          name: photo-album
          env:
            - name: ENDPOINT_URL
              value: 'https://s3-openshift-storage.apps.cluster-7c31.7c31.sandbox905.opentlc.com'
            - name: BUCKET_NAME
              valueFrom:
                configMapKeyRef:
                  name: photo-album
                  key: BUCKET_NAME
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: photo-album
                  key: AWS_ACCESS_KEY_ID
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: photo-album
                  key: AWS_SECRET_ACCESS_KEY
[...]</code></pre>
</div>
</div>
<div class="paragraph">
<p>In order to create objects in your new bucket you must first find the route for the <code>photo-album</code> application.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get route photo-album -n demo -o jsonpath --template="http://{.spec.host}{'\n'}"</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>http://photo-album.apps.cluster-7c31.7c31.sandbox905.opentlc.com</pre>
</div>
</div>
<div class="paragraph">
<p>Copy and paste this route into a web browser tab.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/photo-album-select-upload.png" alt="Select Photo and Upload">
</div>
<div class="title">Figure 28. Select Photo and Upload</div>
</div>
<div class="paragraph">
<p>Select one or more photos of your choosing on your local machine. Then make sure to click the <code>Upload</code> button for each photo.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/photo-album-images.png" alt="View photos after uploading">
</div>
<div class="title">Figure 29. View photos after uploading</div>
</div>
<div class="paragraph">
<p>To view the photos in your object bucket navigate to the <code>MCG Console</code> by viewing the <strong>Object Service</strong> dashboard viewed previously. Select the <code>Mulitcloud Object Gateway</code> link under <code>System Name</code>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/System-Name-MCG-Console.png" alt="Launch MCG console from Object Service dashboard">
</div>
<div class="title">Figure 30. Launch MCG console from Object Service dashboard</div>
</div>
<div class="paragraph">
<p>Login to the <code>MCG Console</code> using <code>username</code> kubeadmin and your <code>password</code>. You can navigate to the bucket details by selecting the <code>Buckets</code> on the far right side. Now select <code>Object Buckets</code>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/MCG-Console-photo-album-buckets.png" alt="Login to MCG Console and select Buckets">
</div>
<div class="title">Figure 31. Login to MCG Console and select Buckets</div>
</div>
<div class="paragraph">
<p>Select your bucket name under <code>Object Buckets</code> and then select the <code>Objects</code> tab to view the individual objects create when you uploaded your photos.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/MCG-Console-photo-album-objects.png" alt="Validate uploaded photos are in your object bucket">
</div>
<div class="title">Figure 32. Validate uploaded photos are in your Object Bucket</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_adding_storage_to_the_ceph_cluster"><a class="anchor" href="#_adding_storage_to_the_ceph_cluster"></a>8. Adding storage to the Ceph Cluster</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Adding storage to OCS adds capacity and performance to your already present
cluster.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The reason for adding more OCP worker nodes for storage is because the
existing nodes do not have adequate CPU and/or Memory available.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_add_storage_worker_nodes"><a class="anchor" href="#_add_storage_worker_nodes"></a>8.1. Add storage worker nodes</h3>
<div class="paragraph">
<p>This section will explain how one can add more worker nodes to the present
storage cluster. Afterwards follow the next sub-section on how to extend the
OCS cluster to provision storage on these new nodes.</p>
</div>
<div class="paragraph">
<p>To add more nodes, we could either add more <strong>machinesets</strong> like we did before,
or scale the already present OCS <strong>machinesets</strong>. For this training, we will
spawn more workers by scaling the already present OCS worker instances up from 1 to 2 <strong>machines</strong>.</p>
</div>
<div class="listingblock execute">
<div class="title">Check on our current workerocs <strong>machinesets</strong> and <strong>machine</strong> counts:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get machinesets -n openshift-machine-api | egrep 'NAME|workerocs'</code></pre>
</div>
</div>
<div class="paragraph">
<p>Example output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>NAME                                           DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-ocs4-8613-bc282-workerocs-us-east-2a   1         1         1       1           2d
cluster-ocs4-8613-bc282-workerocs-us-east-2b   1         1         1       1           2d
cluster-ocs4-8613-bc282-workerocs-us-east-2c   1         1         1       1           2d</pre>
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s scale the workerocs machinesets up with this command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get machinesets -n openshift-machine-api -o name | grep workerocs | xargs -n1 -t oc scale -n openshift-machine-api --replicas=2</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>oc scale -n openshift-machine-api --replicas=2 machineset.machine.openshift.io/cluster-ocs4-8613-bc282-workerocs-us-east-2a
machineset.machine.openshift.io/cluster-ocs4-8613-bc282-workerocs-us-east-2a scaled
oc scale -n openshift-machine-api --replicas=2 machineset.machine.openshift.io/cluster-ocs4-8613-bc282-workerocs-us-east-2b
machineset.machine.openshift.io/cluster-ocs4-8613-bc282-workerocs-us-east-2b scaled
oc scale -n openshift-machine-api --replicas=2 machineset.machine.openshift.io/cluster-ocs4-8613-bc282-workerocs-us-east-2c
machineset.machine.openshift.io/cluster-ocs4-8613-bc282-workerocs-us-east-2c scaled</pre>
</div>
</div>
<div class="paragraph">
<p>Wait until the new OCP workers are available. This could take 5 minutes or more
so be patient. You will know the new OCP worker nodes are available when you
have the number <code>2</code> in all columns.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">watch "oc get machinesets -n openshift-machine-api | egrep 'NAME|workerocs'"</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can exit by pressing <span class="keyseq"><kbd>Ctrl</kbd>+<kbd>C</kbd></span>.</p>
</div>
<div class="paragraph">
<p>Once they are available, you can check to see if the new OCP worker nodes have
the OCS label applied. The total of OCP nodes with the OCS label should now be
six.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The OCS label <code>cluster.ocs.openshift.io/openshift-storage=</code> is already
applied because it is configured in the workerocs <strong>machinesets</strong> that you used
to create the new worker nodes.
</td>
</tr>
</table>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get nodes -l cluster.ocs.openshift.io/openshift-storage -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}'</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>ip-10-0-147-230.us-east-2.compute.internal
ip-10-0-157-22.us-east-2.compute.internal
ip-10-0-175-8.us-east-2.compute.internal
ip-10-0-183-84.us-east-2.compute.internal
ip-10-0-209-53.us-east-2.compute.internal
ip-10-0-214-36.us-east-2.compute.internal</pre>
</div>
</div>
<div class="paragraph">
<p>Now that you have the new instances created with the OCS label, the next step
is to add more storage to the Ceph cluster. The OCS operator will prefer the
new OCP nodes with the OCS label because they have no OCS <strong>Pods</strong> scheduled yet.</p>
</div>
</div>
<div class="sect2">
<h3 id="_add_storage_capacity"><a class="anchor" href="#_add_storage_capacity"></a>8.2. Add storage capacity</h3>
<div class="paragraph">
<p>In this section we will add storage capacity and performance to the
configured OCS worker nodes and the Ceph cluster. If you have followed the
previous section you should now have 6 OCS nodes.</p>
</div>
<div class="paragraph">
<p>To add storage, go to the <strong>Openshift Web Console</strong> and follow these steps to
reach the OCS storage cluster overview:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Click on <code>Operators</code> on the left navigation bar</p>
</li>
<li>
<p>Select <code>Installed Operators</code> and select <code>openshift-storage</code> project</p>
</li>
<li>
<p>Click on <code>Openshift Container Storage Operator</code></p>
</li>
<li>
<p>In the top navigation bar, scroll right to find the item <code>Storage Cluster</code> and click on it</p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-OCP4-Storage-Cluster-overview-reachit.png" alt="OCS4 OCP4 Storage Cluster overview reachit">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>The visible list should list only one item - click on the three dots on the far right to extend the options menu</p>
</li>
<li>
<p>Select <code>Add Capacity</code> from the options menu</p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCS4-add-capacity.png" alt="Add Capacity dialog">
</div>
<div class="title">Figure 33. Add Capacity dialog</div>
</div>
<div class="paragraph">
<p>The storage class should be set to <code>gp2</code>. The added provisioned capacity will
be three times as much as you see in the <code>Raw Capacity</code> field, because OCS uses
a replica count of 3.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<strong>The size chosen for OCS Service Capacity during the initial deployment of OCS is greyed out and cannot be changed.</strong>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Once you are done with your setting, proceed by clicking on <code>Add</code>. You will
see the Status of the Storage Cluster is <code>Ready</code>.</p>
</div>
<div class="admonitionblock caution">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
It may take more than 5 minutes for new OSD pods to be in a <code>Running</code> state.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Use this command to see the new OSD pods:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pod -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName -n openshift-storage | grep osd | grep -v prepare</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>rook-ceph-osd-0-7d45696497-jwgb7            Running     ip-10-0-147-230.us-east-
2.compute.internal
rook-ceph-osd-1-6f49b665c7-gxq75            Running     ip-10-0-209-53.us-east-2
.compute.internal
rook-ceph-osd-2-76ffc64cd-9zg65             Running     ip-10-0-175-8.us-east-2.
compute.internal
rook-ceph-osd-3-97b5d9844-jpwgm             Running     ip-10-0-157-22.us-east-2
.compute.internal
rook-ceph-osd-4-9cb667b76-mftt9             Running     ip-10-0-214-36.us-east-2
.compute.internal
rook-ceph-osd-5-55b8d97855-2bp85            Running     ip-10-0-157-22.us-east-2
.compute.internal</pre>
</div>
</div>
<div class="paragraph">
<p>This is everything that you need to do to extend the OCS storage.</p>
</div>
</div>
<div class="sect2">
<h3 id="_verify_new_storage"><a class="anchor" href="#_verify_new_storage"></a>8.3. Verify new storage</h3>
<div class="paragraph">
<p>Once you added the capacity and made sure that the OSD pods are present, you
can also optionally check the additional storage capacity using the Ceph <strong>toolbox</strong> created earlier. Follow these steps:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD</code></pre>
</div>
</div>
<div class="listingblock execute">
<div class="title">Check the status of the Ceph cluster:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph status</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>sh-4.2# ceph status
  cluster:
    id:     e3398039-f8c6-4937-ba9d-655f5c01e0ae
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum a,b,c (age 25m)
    mgr: a(active, since 24m)
    mds: ocs-storagecluster-cephfilesystem:1 {0=ocs-storagecluster-cephfilesystem-a=up:active} 1 up:standby-replay
    osd: 6 osds: 6 up (since 38s), 6 in (since 38s) <i class="conum" data-value="1"></i><b>(1)</b>

  task status:
      scrub status:
          mds.ocs-storagecluster-cephfilesystem-a: idle
          mds.ocs-storagecluster-cephfilesystem-b: idle

  data:
    pools:   3 pools, 192 pgs
    objects: 92 objects, 81 MiB
    usage:   6.1 GiB used, 12 TiB / 12 TiB avail <i class="conum" data-value="2"></i><b>(2)</b>
    pgs:     192 active+clean

  io:
    client:   1.2 KiB/s rd, 1.7 KiB/s wr, 2 op/s rd, 0 op/s wr</pre>
</div>
</div>
<div class="paragraph">
<p>In the Ceph status output, we can already see that:</p>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>We now use 6 osds in total and they are <code>up</code> and <code>in</code> (meaning the daemons are running and being used to store data)</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The available raw capacity has increased from 6 TiB to 12 TiB</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Besides that, nothing has changed in the output.</p>
</div>
<div class="listingblock execute">
<div class="title">Check the topology of your cluster:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd crush tree</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>ID  CLASS WEIGHT   TYPE NAME
 -1       12.00000 root default
 -5       12.00000     region us-east-2
 -4        4.00000         zone us-east-2a
 -3        2.00000             host ocs-deviceset-gp2-0-data-0-9977n
  0   ssd  2.00000                 osd.0
-21        2.00000             host ocs-deviceset-gp2-2-data-1-nclgr <i class="conum" data-value="1"></i><b>(1)</b>
  4   ssd  2.00000                 osd.4
-14        4.00000         zone us-east-2b
-13        2.00000             host ocs-deviceset-gp2-1-data-0-nnmpv
  2   ssd  2.00000                 osd.2
-19        2.00000             host ocs-deviceset-gp2-0-data-1-mg987 <i class="conum" data-value="1"></i><b>(1)</b>
  3   ssd  2.00000                 osd.3
-10        4.00000         zone us-east-2c
 -9        2.00000             host ocs-deviceset-gp2-2-data-0-mtbtj
  1   ssd  2.00000                 osd.1
-17        2.00000             host ocs-deviceset-gp2-0-data-2-l8tmb <i class="conum" data-value="1"></i><b>(1)</b>
  5   ssd  2.00000                 osd.5</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>We now have additional hosts, which are extending the storage in the respective zone.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Since our Ceph cluster&#8217;s CRUSH rules are set up to replicate data between the
zones, this is an effective way to reduce the load on the 3 initial nodes.</p>
</div>
<div class="paragraph">
<p>Existing data on the original OSDs will be balanced out automatically, so
that the old and the new OSDs share the load.</p>
</div>
<div class="paragraph">
<p>You can exit the <strong>toolbox</strong> by either pressing <span class="keyseq"><kbd>Ctrl</kbd>+<kbd>D</kbd></span> or by executing exit.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">exit</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_monitoring_the_ocs_environment"><a class="anchor" href="#_monitoring_the_ocs_environment"></a>9. Monitoring the OCS environment</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This section covers the different tools available when it comes to monitoring
OCS the environment. This section relies on using the <strong>OpenShift Web Console</strong>.</p>
</div>
<div class="paragraph">
<p>Individuals already familiar with OCP will feel comfortable with this section
but for those who are not, it will be a good primer.</p>
</div>
<div class="paragraph">
<p>The monitoring tools are accessible through the main <strong>OpenShift Web Console</strong>
left pane. Click the <strong>Monitoring</strong> menu item to expand and have access to the
following 3 selections:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Alerting</p>
</li>
<li>
<p>Metrics</p>
</li>
<li>
<p>Dashboards</p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="_alerting"><a class="anchor" href="#_alerting"></a>9.1. Alerting</h3>
<div class="paragraph">
<p>Click on the <strong>Alerting</strong> item to open the Alert window as illustrated in the
screen capture below.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/metrics-alertingleftpanemenu.png" alt="OCP Monitoring Menu">
</div>
<div class="title">Figure 34. OCP Monitoring Menu</div>
</div>
<div class="paragraph">
<p>This will take you to the <strong>Alerting</strong> homepage as illustrated below.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/metrics-alertinghomepage.png" alt="OCP Alerting Homepage">
</div>
<div class="title">Figure 35. OCP Alerting Homepage</div>
</div>
<div class="paragraph">
<p>You can display the alerts in the main window using the filters at your
disposal.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>1 - Will let you select alerts by State, Severity and Sourc</p>
</li>
<li>
<p>2 - Will let you select if you want to search a specific character string
using either the <code>Name</code> or the <code>Label</code></p>
</li>
<li>
<p>3 - Will let you enter the character string you are searching for</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The alert <code>State</code> can be.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>Firing</code> - Alert has been confirmed</p>
</li>
<li>
<p><code>Silenced</code> - Alerts that have been silenced while they were in <code>Pending</code> or <code>Firing</code> state</p>
</li>
<li>
<p><code>Pending</code> - Alerts that have been triggered but not confirmed</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
An alert transitions from <code>Pending</code> to <code>Firing</code> state if the alert
persists for more than the amount of time configured in the alert definition
(e.g. 10 minutes for the <code>CephClusterWarningState</code> alert).
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The alert <code>Severity</code> can be.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>Critical</code> - Alert is tagged as critical</p>
</li>
<li>
<p><code>Warning</code> - Alert is tagged as warning</p>
</li>
<li>
<p><code>Info</code> - Alert is tagged as informational</p>
</li>
<li>
<p><code>None</code> - The alert has no <code>Severity</code> assigned</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The alert <code>Source</code> can be.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>Platform</code> - Alert is generated by an OCP component</p>
</li>
<li>
<p><code>User</code> - Alert is generated by a user application</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>As illustrated below, alerts can be filtered precisely using multiple criteria.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/metrics-alertingstatusfilter.png" alt="OCP Alert Status Filtering">
</div>
<div class="title">Figure 36. OCP Alerting Status Filtering</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You can clear all filters to view all the existing alerts.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>If you select <code>View Alerting Rule</code> you will get access to the details of the
rule that triggered the alert. The details include the Prometheus query used
by the alert to perform the detection of the condition.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/metrics-alertingcontextualmenu.png" alt="OCP Alert Contextual Menu">
</div>
<div class="title">Figure 37. OCP Alert Contextual Menu</div>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/metrics-alertingviewrule.png" alt="OCP Alert Detailed Display">
</div>
<div class="title">Figure 38. OCP Alert Detail Display</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If desired, you can click the Prometheus query embedded in the alert.
Doing so will take you to the <strong>Metrics</strong> page where you will be able to
execute the query for the alert and if desired make changes to the rule.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_metrics"><a class="anchor" href="#_metrics"></a>9.2. Metrics</h3>
<div class="paragraph">
<p>Click on the <strong>Metrics</strong> item as illustrated below in the <code>Monitoring</code> menu.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/metrics-metricsleftpanemenu.png" alt="OCP Metrics Menu">
</div>
<div class="title">Figure 39. OCP Metrics Menu</div>
</div>
<div class="paragraph">
<p>This will take you to the <strong>Metrics</strong> homepage as illustrated below.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/metrics-queryfield.png" alt="OCP Monitoring Metrics Homepage">
</div>
<div class="title">Figure 40. OCP Monitoring Metrics Homepage</div>
</div>
<div class="paragraph">
<p>Use the query field to either enter the formula of your choice or to search for
metrics by name. The metrics available will let you query both OCP related
information or OCS related information. The queries can be simple or complex
using the Prometheus query syntax and all its available functions.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s start testing a simple query example and enter the following text
<code>ceph_osd_op</code> in the query field. When you are done typing, simply hit
<code>[Enter]</code> or select <code>Run Queries</code>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/metrics-simplecephquery.png" alt="Ceph Simple Query">
</div>
<div class="title">Figure 41. Simple Ceph Query</div>
</div>
<div class="paragraph">
<p>The window should refresh with a graph similar to the one below.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/metrics-simplecephgraph.png" alt="Ceph Simple Graph">
</div>
<div class="title">Figure 42. Simple Ceph Graph</div>
</div>
<div class="paragraph">
<p>Then let&#8217;s try a more relevant query example and enter the following text
<code>rate(ceph_osd_op[5m])</code> or <code>irate(ceph_osd_op[5m])</code> in the query field. When
you are done typing, simply hit <code>[Enter]`or select `Run Queries</code>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/metrics-complexcephquery.png" alt="Ceph Complex Query">
</div>
<div class="title">Figure 43. Complex Ceph Query</div>
</div>
<div class="paragraph">
<p>The window should refresh with a graph similar to the one below.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/metrics-complexcephgraph.png" alt="Ceph Complex Graph">
</div>
<div class="title">Figure 44. Complex Ceph Graph</div>
</div>
<div class="paragraph">
<p>All OCP metrics are also available through the integrated <strong>Metrics</strong> window.
Feel free to try with any of the OCP related metrics such as
<code>irate(process_cpu_seconds_total[5m])</code> for example.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/metrics-complexocpgraph.png" alt="OCP Complex Graph">
</div>
<div class="title">Figure 45. Complex OCP Graph</div>
</div>
<div class="paragraph">
<p>Have a look at the difference between <code>sum(irate(process_cpu_seconds_total[5m]))</code> and the last query <code>irate(process_cpu_seconds_total[5m])</code>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
For more information on the Prometheus query language visit the
<a href="https://prometheus.io/docs/prometheus/latest/querying/basics/">Prometheus
Query Documentation</a>.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_using_must_gather"><a class="anchor" href="#_using_must_gather"></a>10. Using must-gather</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Must-gather is a tool for collecting data about your running Openshift cluster.
It loads a predefined set of containers that execute multiple programs and
write results on the local workstation&#8217;s filesystem. The local files can then
be uploaded to a Red Hat case and used by a remote support engineer to debug a
problem without needing direct access to your cluster. This utility and method
for diagnostic collection is similar to sosreports for RHEL hosts.</p>
</div>
<div class="paragraph">
<p>The OCS team has released its own must-gather image for the must-gather tool
that runs storage specific commands.</p>
</div>
<div class="paragraph">
<p>You can run this diagnostic tool like this for generic OpenShift debugging:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc adm must-gather</code></pre>
</div>
</div>
<div class="paragraph">
<p>Or like this for OCS specific results:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc adm must-gather --image=registry.redhat.io/ocs4/ocs-must-gather-rhel8:v4.6</code></pre>
</div>
</div>
<div class="paragraph">
<p>The output will then be saved in the current directory inside of a new folder
called <code>must-gather.local.(random)</code></p>
</div>
<div class="paragraph">
<p>More runtime options can be displayed with this command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc adm must-gather -h</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>Launch a pod to gather debugging information

 This command will launch a pod in a temporary namespace on your cluster that gathers debugging information and then
downloads the gathered information.

 Experimental: This command is under active development and may change without notice.

Usage:
  oc adm must-gather [flags]

Examples:
  # gather information using the default plug-in image and command, writing into ./must-gather.local.&lt;rand&gt;
  oc adm must-gather

  # gather information with a specific local folder to copy to
  oc adm must-gather --dest-dir=/local/directory

  # gather information using multiple plug-in images
  oc adm must-gather --image=quay.io/kubevirt/must-gather --image=quay.io/openshift/origin-must-gather

  # gather information using a specific image stream plug-in
  oc adm must-gather --image-stream=openshift/must-gather:latest

  # gather information using a specific image, command, and pod-dir
  oc adm must-gather --image=my/image:tag --source-dir=/pod/directory -- myspecial-command.sh

Options:
      --dest-dir='': Set a specific directory on the local machine to write gathered data to.
      --image=[]: Specify a must-gather plugin image to run. If not specified, OpenShift's default must-gather image
will be used.
      --image-stream=[]: Specify an image stream (namespace/name:tag) containing a must-gather plugin image to run.
      --node-name='': Set a specific node to use - by default a random master will be used
      --source-dir='/must-gather/': Set the specific directory on the pod copy the gathered data from.

Use "oc adm options" for a list of global command-line options (applies to all commands).</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_configuring_the_local_oc_client"><a class="anchor" href="#_configuring_the_local_oc_client"></a>Appendix A: Configuring the local oc client</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This section will explain how you set up your workstation with the Openshift CLI.</p>
</div>
<div class="paragraph">
<p>To get the latest OpenShift CLI client run the following commands:</p>
</div>
<div class="listingblock">
<div class="title">Mac steps</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">curl -s https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/openshift-client-mac.tar.gz | tar xzv oc
sudo mv oc /usr/local/bin</code></pre>
</div>
</div>
<div class="paragraph">
<p>In addition install the watch command to use with the <code>oc client</code> on your Mac using Homebrew.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>brew install watch</pre>
</div>
</div>
<div class="paragraph">
<p>Alternatively, add the following lines to your $HOME/.profile.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>function watch {
while :; do clear; date; echo; $@; sleep 2; done
}</pre>
</div>
</div>
<div class="paragraph">
<p>Then reload your profile with <code>source $HOME/.profile</code>.</p>
</div>
<div class="listingblock">
<div class="title">Linux steps</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">curl -s https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/openshift-client-linux.tar.gz | tar xzv oc
sudo mv oc /usr/bin</code></pre>
</div>
</div>
<div class="paragraph">
<p>Afterwards, go to your <strong>Openshift Web Console</strong>, log in and click on the
username in the top right corner. There you will find the menu item <code>Copy Login
Command</code>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/OCP-copy-login-command.png" alt="OCP copy login command">
</div>
<div class="title">Figure 46. Copy Login command menu entry</div>
</div>
<div class="paragraph">
<p>Clicking on <code>Copy Login Command</code> will open a new window and you might be
required to login to your Openshift cluster again. After successfully login in,
you see blue text <code>Display token</code> - click this and you will be shown your login
command.</p>
</div>
<div class="paragraph">
<p>The login command will look similar to this:</p>
</div>
<div class="paragraph">
<p><code>oc login --token=zoNoANLOOoJzXV3sb-TE1xIcg2aLBssdN0bTNIuV29w
--server=https://api.cluster-ocs-89db.ocs-89db.example.opentlc.com:6443</code></p>
</div>
<div class="paragraph">
<p>Execute this login command on your terminal. If you did not provision your
RHPDS environment with Let&#8217;s Encrypt certificates, it will ask if you want to
connect without certificate checks, accept this.</p>
</div>
<div class="paragraph">
<p>Check if you are successfully connected by issuing a command against the
cluster:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc version</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>Client Version: 4.6.12
Server Version: 4.6.12
Kubernetes Version: v1.19.0+7070803</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The above commands will always pull the latest oc version, so your
version might be higher than the version in the example output.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_install_the_noobaa_cli_client"><a class="anchor" href="#_install_the_noobaa_cli_client"></a>Appendix B: Install the NooBaa CLI client</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To install the CLI, follow these steps on your workstation:</p>
</div>
<div class="listingblock">
<div class="title">Mac steps</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">brew install noobaa/noobaa/noobaa</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Mac steps without Homebrew</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">curl -sLO https://github.com/noobaa/noobaa-operator/releases/download/v5.6.0/noobaa-mac-v5.6.0 ; chmod +x noobaa-mac-v5.6.0 ; sudo mv noobaa-mac-v5.6.0 /usr/local/bin/noobaa</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Linux steps</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">curl -sLO https://github.com/noobaa/noobaa-operator/releases/download/v5.6.0/noobaa-linux-v5.6.0 ; chmod +x noobaa-linux-v5.6.0 ; sudo mv noobaa-linux-v5.6.0 /usr/bin/noobaa</code></pre>
</div>
</div>
<div class="paragraph">
<p>Check that your noobaa CLI installation was successful with this command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">noobaa version</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>INFO[0000] CLI version: 5.6.0
INFO[0000] noobaa-image: noobaa/noobaa-core:5.6.0
INFO[0000] operator-image: noobaa/noobaa-operator:5.6.0</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_introduction_to_ceph"><a class="anchor" href="#_introduction_to_ceph"></a>Appendix C: Introduction to Ceph</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This section will go through Ceph fundamental knowledge for a better
understanding of the underlying storage solution
used by OCS 4.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The content in this Appendix is relevant to learning about the critical
components of Ceph and how Ceph works. OCS 4 uses Ceph in a prescribed manner
for providing storage to OpenShift applications. Using <strong>Operators</strong> and
<strong>CustomResourceDefinitions</strong> (CRDs) for deploying and managing OCS 4 may
restrict some of Ceph&#8217;s advanced features when compared to general use
outside of OCP 4.
</td>
</tr>
</table>
</div>
<div class="paragraph lead">
<p><strong>Timeline</strong></p>
</div>
<div class="paragraph">
<p>The Ceph project has a long history as you can see in the timeline below.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/ceph101-timeline.png" alt="Ceph Project Timeline">
</div>
<div class="title">Figure 47. Ceph Project History</div>
</div>
<div class="paragraph lead">
<p>It is a battle-tested software defined storage (SDS) solution that has been
available as a storage backend for OpenStack and Kubernetes for quite some
time.</p>
</div>
<div class="paragraph lead">
<p><strong>Architecture</strong></p>
</div>
<div class="paragraph">
<p>The Ceph cluster provides a scalable storage solution while providing
multiple access methods to enable the different types of
clients present within the IT infrastructure to get access to the data.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/ceph101-overview.png" alt="Ceph From Above">
</div>
<div class="title">Figure 48. Ceph Architecture</div>
</div>
<div class="paragraph lead">
<p>The entire Ceph architecture is resilient and does not present any single point
of failure (SPOF).</p>
</div>
<div class="paragraph lead">
<p><strong>RADOS</strong></p>
</div>
<div class="paragraph">
<p>The heart of Ceph is an object store known as RADOS (Reliable Autonomic
Distributed Object Store) bottom layer on the screen. This layer provides the
Ceph software defined storage with the ability to store data (serve IO
requests, to protect the data, to check the consistency and the integrity of
the data through built-in mechanisms. The RADOS layer is composed of the
following daemons:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>MONs or Monitors</p>
</li>
<li>
<p>OSDs or Object Storage Devices</p>
</li>
<li>
<p>MGRs or Managers</p>
</li>
<li>
<p>MDSs or Meta Data Servers</p>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title"><strong><em>Monitors</em></strong></div>
<p>The Monitors maintain the cluster map and state and provide distributed
decision-making while configured in an odd number, 3 or 5 depending on the
size and the topology of the cluster, to prevent split-brain situations. The
Monitors are not in the data-path and do not serve IO requests to and from
the clients.</p>
</div>
<div class="paragraph">
<div class="title"><strong><em>OSDs</em></strong></div>
<p>One OSD is typically deployed for each local block devices and the native
scalable nature of Ceph allows for thousands of OSDs to be part of the
cluster. The OSDs are serving IO requests from the clients while guaranteeing
the protection of the data (replication or erasure coding), the rebalancing
of the data in case of an OSD or a node failure, the coherence of the data
(scrubbing and deep-scrubbing of the existing data).</p>
</div>
<div class="paragraph">
<div class="title"><strong><em>MGRs</em></strong></div>
<p>The Managers are tightly integrated with the Monitors and collect the
statistics within the cluster. Additionally they provide an extensible
framework for the cluster through a pluggable Python interface aimed at
expanding the Ceph existing capabilities. The current list of modules
developed around the Manager framework are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Balancer module</p>
</li>
<li>
<p>Placement Group auto-scaler module</p>
</li>
<li>
<p>Dashboard module</p>
</li>
<li>
<p>RESTful module</p>
</li>
<li>
<p>Prometheus module</p>
</li>
<li>
<p>Zabbix module</p>
</li>
<li>
<p>Rook module</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title"><strong><em>MDSs</em></strong></div>
<p>The Meta Data Servers manage the metadata for the POSIX compliant shared
filesystem such as the directory hierarchy and the file metadata (ownership,
timestamps, mode, &#8230;&#8203;). All the metadata is stored with RADOS and they do not
server any data to the clients. MDSs are only deployed when a shared
filesystem is configured in the Ceph cluster.</p>
</div>
<div class="paragraph">
<p>If we look at the Ceph cluster foundation layer, the full picture with the
different types of daemons or containers looks like this.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/ceph101-rados.png" alt="RADOS Overview">
</div>
<div class="title">Figure 49. RADOS as it stands</div>
</div>
<div class="paragraph">
<p>The circle represent the MONs, the 'M' represent the MGRs and the squares
with the bars represent the OSDs. In the diagram above, the cluster operates
with 3 Monitors, 2 Managers and 23 OSDs.</p>
</div>
<div class="paragraph lead">
<p><strong>Access Methods</strong></p>
</div>
<div class="paragraph">
<p>Ceph was designed to provides the IT environment with all the necessary
access methods so that any application can use what is the best solution for
its use-case.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/ceph101-differentstoragetypes.png" alt="Ceph Access Modes">
</div>
<div class="title">Figure 50. Different Storage Types Supported</div>
</div>
<div class="paragraph">
<p>Ceph supports block storage through the RADOS Block Device (aka RBD) access
method, file storage through the Ceph Filesystem (aka CephFS) access method
and object storage through its native <code>librados</code> API or through the RADOS
Gateway (aka RADOSGW or RGW) for compatibility with the S3 and Swift
protocols.</p>
</div>
<div class="paragraph lead">
<p><strong>Librados</strong></p>
</div>
<div class="paragraph">
<p>Librados allows developers to code natively against the native Ceph cluster
API for maximum efficiency combined with a small footprint.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/ceph101-librados.png" alt="librados">
</div>
<div class="title">Figure 51. Application Native Object API</div>
</div>
<div class="paragraph">
<p>The Ceph native API offers different wrappers such as C, C++, Python, Java,
Ruby, Erlang, Go and Rust.</p>
</div>
<div class="paragraph lead">
<p><strong>RADOS Block Device (RBD)</strong></p>
</div>
<div class="paragraph">
<p>This access method is used in Red Hat Enterprise Linux or OpenShift version
3.x or 4.x. RBDs can be accessed either through a kernel module (RHEL, OCS4)
or through the <code>librbd</code> API (RHOSP). In the OCP world, RBDs are designed to
address the need for RWO PVCs.</p>
</div>
<div class="paragraph lead">
<p><strong><em>Kernel Module (kRBD)</em></strong></p>
</div>
<div class="paragraph">
<p>The kernel RBD driver offers superior performance compared to the userspace
<code>librbd</code> method. However, kRBD is currently limited and does not provide the
same level of functionality. e.g., no RBD Mirroring support.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/ceph101-krbd.png" alt="Kernel based RADOS Block Device">
</div>
<div class="title">Figure 52. kRBD Diagram</div>
</div>
<div class="paragraph lead">
<p><strong><em>Userspace RBD (librbd)</em></strong></p>
</div>
<div class="paragraph">
<p>This access method is used in Red Hat OpenStack Environment or OpenShift
through the RBD-NBD driver when available starting in the RHEL 8.1 kernel.
This mode allows us to leverage all existing RBD features such as RBD
Mirroring.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/ceph101-librbd.png" alt="Userspace RADOS Block Device">
</div>
<div class="title">Figure 53. librbd Diagram</div>
</div>
<div class="paragraph lead">
<p><strong><em>Shared Filesystem (CephFS)</em></strong></p>
</div>
<div class="paragraph">
<p>This method allows clients to jointly access a shared POSIX compliant
filesystem. The client initially contacts the Meta Data Server to obtain the
location of the object(s) for a given inode and then communicates directly
with an OSD to perform the final IO request.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/ceph101-cephfs.png" alt="Kernel Based CephFS Client">
</div>
<div class="title">Figure 54. File Access (Ceph Filesystem or CephFS)</div>
</div>
<div class="paragraph">
<p>CephFS is typically used for RWX claims but can also be used to support RWO claims.</p>
</div>
<div class="paragraph lead">
<p><strong><em>Object Storage, S3 and Swift (Ceph RADOS Gateway)</em></strong></p>
</div>
<div class="paragraph">
<p>This access method offers support for the Amazon S3 and OpenStack Swift
support on top of a Ceph cluster. The Openshift Container Storage Multi Cloud
Gateway can leverage the RADOS Gateway to support Object Bucket Claims. From
the Multi Cloud Gateway perspective the RADOS Gateway will be tagged as a
compatible S3 endpoint.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/ceph101-rgw.png" alt="S3 and Swift Support">
</div>
<div class="title">Figure 55. Amazone S3 or OpenStack Swift (Ceph RADOS Gateway)</div>
</div>
<div class="paragraph lead">
<p><strong>CRUSH</strong></p>
</div>
<div class="paragraph">
<p>The Ceph cluster being a distributed architecture some solution had to be
designed to provide an efficient way to distribute the data across the
multiple OSDs in the cluster. The technique used is called CRUSH or
Controlled Replication Under Scalable Hashing. With CRUSH, every object is
assigned to one and only one hash bucket known as a Placement Group (PG).</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/ceph101-crushfromobjecttoosd.png" alt="From Object to OSD">
</div>
</div>
<div class="paragraph">
<p>CRUSH is the central point of configuration for the topology of the cluster.
It offers a pseudo-random placement algorithm to distribute the objects
across the PGs and uses rules to determine the mapping of the PGs to the
OSDs. In essence, the PGs are an abstraction layer between the objects
(application layer) and the OSDs (physical layer). In case of failure, the
PGs will be remapped to different physical devices (OSDs) and eventually see
their content resynchronized to match the protection rules selected by the
storage administrator.</p>
</div>
<div class="paragraph lead">
<p><strong>Cluster Partitioning</strong></p>
</div>
<div class="paragraph">
<p>The Ceph OSDs will be in charge of the protection of the data as well as the
constant checking of the integrity of the data stored in the entire cluster.
The cluster will be separated into logical partitions, known as pools. Each
pool has the following properties that can be adjusted:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>An ID (immutable)</p>
</li>
<li>
<p>A name</p>
</li>
<li>
<p>A number of PGs to distribute the objects across the OSDs</p>
</li>
<li>
<p>A CRUSH rule to determine the mapping of the PGs for this pool</p>
</li>
<li>
<p>A type of protection (Replication or Erasure Coding)</p>
</li>
<li>
<p>Parameters associated with the type of protection</p>
<div class="ulist">
<ul>
<li>
<p>Number of copies for replicated pools</p>
</li>
<li>
<p>K and M chunks for Erasure Coding</p>
</li>
</ul>
</div>
</li>
<li>
<p>Various flags to influence the behavior of the cluster</p>
</li>
</ul>
</div>
<div class="paragraph lead">
<p><strong>Pools and PGs</strong></p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/ceph101-thefullpicture.png" alt="From Object to OSD">
</div>
<div class="title">Figure 56. Pools and PGs</div>
</div>
<div class="paragraph">
<p>The diagram above shows the relationship end to end between the object at the
access method level down to the OSDs at the physical layer.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
A Ceph pool has no size and is able to consume the space available on any
OSD where it&#8217;s PGs are created. A Placement Group or PG belongs to only one
pool.
</td>
</tr>
</table>
</div>
<div class="paragraph lead">
<p><strong>Data Protection</strong></p>
</div>
<div class="paragraph">
<p>Ceph supports two types of data protection presented in the diagram below.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/ceph101-dataprotection.png" alt="Replicated Pools vs Erasure Coded Pools">
</div>
<div class="title">Figure 57. Ceph Data Protection</div>
</div>
<div class="paragraph">
<p>Replicated pools provide better performance in almost all cases at the cost
of a lower usable to raw storage ratio (1 usable byte is stored using 3 bytes
of raw storage) while <code>Erasure Coding</code> provides a cost efficient way to store
data with less performance. Red Hat supports the following <code>Erasure Coding</code>
profiles with their corresponding usable to raw ratio:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>4+2 (1:2 ratio)</p>
</li>
<li>
<p>8+3 (1:1.375 ratio)</p>
</li>
<li>
<p>8+4 (1:2 ratio)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Another advantage of <code>Erasure Coding</code> (EC) is its ability to offer extreme
resilience and durability as we can configure the number of parities being
used. EC can be used for the RADOS Gateway access method and for the RBD
access method (performance impact).</p>
</div>
<div class="paragraph lead">
<p><strong>Data Distribution</strong></p>
</div>
<div class="paragraph">
<p>To leverage the Ceph architecture at its best, all access methods but
librados, will access the data in the cluster through a collection of
objects. Hence a 1GB block device will be a collection of objects, each
supporting a set of device sectors. Therefore, a 1GB file is stored in a
CephFS directory will be split into multiple objects. Also a 5GB S3 object
stored through the RADOS Gateway via the Multi Cloud Gateway will be divided
in multiple objects.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/ceph101-rbdlayout.png" alt="RADOS Block Device Layout">
</div>
<div class="title">Figure 58. Data Distribution</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
By default, each access method uses an object size of 4MB. The above
diagram details how a 32MB RBD (Block Device) supporting a RWO PVC will be
scattered throughout the cluster.
</td>
</tr>
</table>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">
      <img src="../../_/img/header_logo.svg" alt="Red Hat Data Services">
  </a>
</footer>
<script src="../../_/js/site.js"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
